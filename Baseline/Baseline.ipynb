{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import multiprocessing\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.backend.clear_session()\n",
    "sess = tf.Session( config = tf.ConfigProto( device_count = {'gpu':0} ) )\n",
    "KTF.set_session( sess )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get data from path\n",
    "\n",
    "Args:\n",
    "    path: a string represents corpus path of each language.\n",
    "    language_list: a list of string represents languages.\n",
    "    encoding_list: a list of string represents encoding of each language\n",
    "                   corresponding to language list.\n",
    "    shuffle: a boolean value. True for shuffle.\n",
    "\n",
    "Returns:\n",
    "    lan_data: a dictionary contains sentences of each language.\n",
    "              Its structure is:\n",
    "              \n",
    "              {language A: [[word1, word2, ...], [...], ...],\n",
    "               language B: ...}\n",
    "\"\"\"\n",
    "def get_data( path = \"../data/\", language_list = [\"chinese\", \"english\"],\n",
    "              encoding_list = [\"UTF-8\", \"UTF-8\"], shuffle = True ):\n",
    "    assert len( language_list ) == len( encoding_list )\n",
    "    # Just for my convenient in the following\n",
    "    lan_list, enc_list = language_list, encoding_list\n",
    "    \n",
    "    # Read parallel corpus\n",
    "    lan_data = {}\n",
    "    for i in range( len( lan_list ) ):\n",
    "        lan = lan_list[i]\n",
    "        print( \"Reading \" + lan + \" language corpus...\" )\n",
    "        if lan not in lan_data:\n",
    "            lan_data[lan] = []\n",
    "        files = os.listdir( path + lan + \"/\" )\n",
    "        for file in files:\n",
    "            with open( path + lan + \"/\" + file, \"r\", encoding = enc_list[i] ) as f:\n",
    "                line = f.readline()\n",
    "                while line:\n",
    "                    line = line.strip()\n",
    "                    if len( line ) == 0:\n",
    "                        line = f.readline()\n",
    "                        continue\n",
    "                    words = [\"<S>\"] + line.split() + [\"</S>\"]\n",
    "                    lan_data[lan].append( words )\n",
    "                    line = f.readline()\n",
    "    \n",
    "    if shuffle == True:\n",
    "        print( \"Shuffling...\" )\n",
    "        \n",
    "        # Decide shuffle order\n",
    "        length = len( lan_data[lan_list[0]] )\n",
    "        shuf_list = [i for i in range( length )]\n",
    "        random.shuffle( shuf_list )\n",
    "\n",
    "        # Shuffle corpus\n",
    "        for lan in lan_list:\n",
    "            lan_data[lan] = np.array( lan_data[lan] )[shuf_list].tolist()\n",
    "    \n",
    "    return lan_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save dictionary to file\n",
    "\n",
    "Args:\n",
    "    dt: the dictionary to save.\n",
    "    file_name: the name of file to save.\n",
    "    path: the path of file to save.\n",
    "\n",
    "Returns:\n",
    "    None.\n",
    "\"\"\"\n",
    "def save_dict( dt, file_name, path = \"dicts/\" ):\n",
    "    print( \"Saving to \" + path + file_name + \"...\" )\n",
    "    fileName = path + file_name\n",
    "    with open( file_name, \"w\" ) as f:\n",
    "        jsn = json.dumps( dt )\n",
    "        f.write( jsn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sort dictionary by length of original language\n",
    "\n",
    "Args:\n",
    "    data: a dictionary contains sentences of each language.  Its structure is:\n",
    "    \n",
    "          {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \n",
    "Returns:\n",
    "    None.\n",
    "\"\"\"\n",
    "def sort_by_ori_lan( data ):\n",
    "    print( \"Sorting...\" )\n",
    "    lan = list( data.keys() )\n",
    "    tmp = list( zip( data[lan[0]], data[lan[1]] ) )\n",
    "    tmp.sort( key = lambda x: len( x[0] ) )\n",
    "    data[lan[0]], data[lan[1]] = zip( *tmp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build dictionary for each language\n",
    "\n",
    "Args:\n",
    "    language_data: a dictionary contains sentences of each language.\n",
    "                   Its structure is:\n",
    "                   \n",
    "                   {language A: [[word1, word2, ...], [...], ...],\n",
    "                    language B: ...}\n",
    "    threshold: a integer represents threshold. If the number of a word\n",
    "               is less than threshold, it will be replaced by <UNK>.\n",
    "\n",
    "Returns:\n",
    "    word_to_idx_dict: a dictionary converts word to index. Its structure is:\n",
    "                      \n",
    "                      {language A: {word A: index A, word B: ..., ...},\n",
    "                       language B: ...}.\n",
    "\n",
    "    idx_to_word_dict: a dictionary converts index to word. Its structure is:\n",
    "                      \n",
    "                      {language A: {index A: word A, index B: ..., ...},\n",
    "                       language B: ...}.\n",
    "\"\"\"\n",
    "def build_dictionary( language_data, threshold = 0 ):\n",
    "    lan_data = language_data\n",
    "    word_to_idx_dict = {}\n",
    "    idx_to_word_dict = {}\n",
    "    for lan, sentences in lan_data.items():\n",
    "        # Generate dictionary for each language\n",
    "        if lan not in word_to_idx_dict:\n",
    "            word_to_idx_dict[lan] = {\"<PAD>\": 0, \"<S>\": 1, \"</S>\": 2, \"<UNK>\": 3}\n",
    "        if lan not in idx_to_word_dict:\n",
    "            idx_to_word_dict[lan] = {0: \"<PAD>\", 1: \"<S>\", 2: \"</S>\", 3: \"<UNK>\"}\n",
    "        \n",
    "        # Count words\n",
    "        word_count = {}\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in word_count:\n",
    "                    word_count[word] = 0\n",
    "                word_count[word] += 1\n",
    "        \n",
    "        # Replace words to <UNK>\n",
    "        for word, count in word_count.items():\n",
    "            if count <= threshold:\n",
    "                word = \"<UNK>\"\n",
    "            if word not in word_to_idx_dict[lan]:\n",
    "                idx = len( word_to_idx_dict[lan] )\n",
    "                word_to_idx_dict[lan][word] = idx\n",
    "                idx_to_word_dict[lan][idx] = word\n",
    "                \n",
    "    return word_to_idx_dict, idx_to_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_seq2seq( input_vocab_size, output_vocab_size,\n",
    "                    hidden_dim = 128, word_vec_dim = 300,\n",
    "                    name = \"baseline\" ):\n",
    "    ### Encoder-Decoder for train ###\n",
    "    \n",
    "    # Encoder\n",
    "    encoder_embedding = Embedding( output_dim = word_vec_dim,\n",
    "                                   input_dim  = input_vocab_size,\n",
    "                                   mask_zero  = True,\n",
    "                                   name = name + \"_encoder_embedding\")\n",
    "    encoder = LSTM( hidden_dim, return_state = True,\n",
    "                    name = name + \"_encoder_lstm\" )\n",
    "    encoder_input = Input( shape = ( None, ),\n",
    "                           name = name + \"_encoder_input\" )\n",
    "    \n",
    "    encoder_input_emb   = encoder_embedding( encoder_input )\n",
    "    _, state_h, state_c = encoder( encoder_input_emb )\n",
    "    encoder_state       = [state_h, state_c]\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_embedding = Embedding( output_dim = word_vec_dim,\n",
    "                                   input_dim = output_vocab_size,\n",
    "                                   mask_zero = True,\n",
    "                                   name = name + \"_decoder_embedding\")\n",
    "    decoder = LSTM( hidden_dim, return_state = True, return_sequences = True,\n",
    "                    name = name + \"_decoder_lstm\" )\n",
    "    decoder_dense = Dense( output_vocab_size, activation = \"softmax\",\n",
    "                           name = name + \"_decoder_output\" )\n",
    "    decoder_input = Input( shape = ( None, ),\n",
    "                           name = name + \"_decoder_input\" )\n",
    "    \n",
    "    decoder_input_emb = decoder_embedding( decoder_input )\n",
    "    decoder_output, state_h, state_c = decoder( decoder_input_emb,\n",
    "                                                initial_state = encoder_state )\n",
    "    decoder_output    = decoder_dense( decoder_output )\n",
    "    \n",
    "    # Model\n",
    "    model = Model( inputs = [encoder_input, decoder_input],\n",
    "                   outputs = decoder_output,\n",
    "                   name = name )\n",
    "    model.compile( optimizer = 'adam', loss = \"categorical_crossentropy\" )\n",
    "    \n",
    "    ### Encoder-Decoder for generation ###\n",
    "    \n",
    "    # Encoder Model\n",
    "    encoder_model   = Model( inputs  = encoder_input,\n",
    "                             outputs = encoder_state,\n",
    "                             name = name + \"_encoder\" )\n",
    "    \n",
    "    # Decoder Model\n",
    "    decoder_state_h = Input( shape = ( hidden_dim, ), name = name + \"_state_h\" )\n",
    "    decoder_state_c = Input( shape = ( hidden_dim, ), name = name + \"_state_c\" )\n",
    "    decoder_state_input = [decoder_state_h, decoder_state_c]\n",
    "    decoder_output, state_h, state_c = decoder( decoder_input_emb,\n",
    "                                                initial_state = decoder_state_input )\n",
    "    decoder_state   = [state_h, state_c]\n",
    "    decoder_output  = decoder_dense( decoder_output )\n",
    "    decoder_model   = Model( inputs  = [decoder_input] + decoder_state_input,\n",
    "                             outputs = [decoder_output] + decoder_state,\n",
    "                             name = name + \"_decoder\" )\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate sentences based on given sentences\n",
    "\n",
    "Args:\n",
    "    data: a list of dev data sentences. Its structure is:\n",
    "    \n",
    "          [[word1, word2, ...], [...], ...]\n",
    "\n",
    "    encoder_model: encoder part of seq2seq model.\n",
    "    decoder_model: decoder (generate) part of seq2se1 model.\n",
    "    max_len: a interger represents the max length of generated (translated)\n",
    "             sentence.\n",
    "    word_to_idx_dict: a dictionary of original language converts word to index.\n",
    "                      Its structure is:\n",
    "                      \n",
    "                      {word A: index A, word B: ..., ...}\n",
    "                       \n",
    "    idx_to_word_dict: a dictionary of target language converts index to word.\n",
    "                      Its structure is:\n",
    "\n",
    "                      {index A: word A, index B: ..., ...}.\n",
    "\n",
    "Returns:\n",
    "    sentences: a list of generated (translated) sentences.\n",
    "\"\"\"\n",
    "def translate_sentences(data, encoder_model, decoder_model, max_len, word_to_idx_dict, idx_to_word_dict):\n",
    "    sentences = []\n",
    "    for sentence in data:\n",
    "        init = \"<S>\"\n",
    "        cnt = 0\n",
    "        words = []\n",
    "        sentence_ = [word_to_idx_dict[x] if x in word_to_idx_dict else 3 for x in sentence]\n",
    "        state = encoder_model.predict(sentence_)\n",
    "        while init != \"</S>\" and cnt <= max_len + 1:\n",
    "            print( init )\n",
    "            index = np.array([word_to_idx_dict[init]]).reshape( ( 1, 1 ) )\n",
    "            print( index )\n",
    "            indeces, state_h, state_c = decoder_model.predict([index] + state)\n",
    "            index = np.argmax(indeces[0, -1, :]) # please check indeces.shape at first\n",
    "            init = idx_to_word_dict[index]\n",
    "            print(init)\n",
    "            state = [state_h, state_c]\n",
    "            words.append(init)\n",
    "            cnt += 1\n",
    "        sentences.append(words[:-1])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_index( data, word_to_idx_dict ):\n",
    "    print( \"Coverting to index...\" )\n",
    "    lan_list = list( data.keys() )\n",
    "    for lan in lan_list:\n",
    "        for sentence in data[lan]:\n",
    "            for i in range( len( sentence ) ):\n",
    "                word = sentence[i]\n",
    "                if word not in word_to_idx_dict[lan]:\n",
    "                    word = \"<UNK>\"\n",
    "                sentence[i] = word_to_idx_dict[lan][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_batch( data, batch_size = 64, max_length = 32 ):\n",
    "    print( \"Converting to batches...\" )\n",
    "    tdata = {}\n",
    "    lan_list = list( data.keys() )\n",
    "    for lan in lan_list:\n",
    "        if lan not in tdata:\n",
    "            tdata[lan] = []\n",
    "    for i in range( 0, len( data[lan_list[0]] ) + batch_size, batch_size ):\n",
    "        lan0 = data[lan_list[0]][i: i + batch_size]\n",
    "        lan1 = data[lan_list[1]][i: i + batch_size]\n",
    "        # Calculate max length of valid sentences\n",
    "        n = 0\n",
    "        max_len0, max_len1 = 0, 0\n",
    "        for j in range( len( lan0 ) ):\n",
    "            len0 = len( lan0[j] )\n",
    "            len1 = len( lan1[j] )\n",
    "            if len0 <= max_length:\n",
    "                max_len0 = max( max_len0, len0 )\n",
    "                max_len1 = max( max_len1, len1 )\n",
    "                n += 1\n",
    "        # If there is no sentence valid, ignore the batch\n",
    "        if n == 0:\n",
    "            continue\n",
    "        # Convert to batch\n",
    "        np_lan0 = np.zeros( ( n, max_len0 ) )\n",
    "        np_lan1 = np.zeros( ( n, max_len1 ) )\n",
    "        n = 0\n",
    "        for j in range( len( lan0 ) ):\n",
    "            len0 = len( lan0[j] )\n",
    "            if len0 <= max_length:\n",
    "                for k in range( len( lan0[j] ) ):\n",
    "                    np_lan0[n, k] = lan0[j][k]\n",
    "                for k in range( len( lan1[j] ) ):\n",
    "                    np_lan1[n, k] = lan1[j][k]\n",
    "                n += 1\n",
    "        tdata[lan_list[0]].append( np_lan0 )\n",
    "        tdata[lan_list[1]].append( np_lan1 )\n",
    "    return tdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"baseline\"\n",
    "language_list = [\"chinese\", \"english\"] # [ori_lan, tar_lan]\n",
    "batch_size = 64\n",
    "max_length = 32\n",
    "\n",
    "data = get_data( \"../data/train/\" )\n",
    "word_to_idx_dict, idx_to_word_dict = build_dictionary( data )\n",
    "save_dict( word_to_idx_dict, \"word_to_idx.json\" )\n",
    "save_dict( idx_to_word_dict, \"idx_to_word.json\" )\n",
    "input_vocab_size = len( word_to_idx_dict[language_list[0]] )\n",
    "output_vocab_size = len( word_to_idx_dict[language_list[1]] )\n",
    "\n",
    "print( \"Building Model\" )\n",
    "model, encoder_model, decoder_model = simple_seq2seq( input_vocab_size,\n",
    "                                                      output_vocab_size,\n",
    "                                                      name = model_name )\n",
    "\n",
    "sort_by_ori_lan( data )\n",
    "convert_to_index( data, word_to_idx_dict )\n",
    "data = convert_to_batch( data )\n",
    "print( len( data[language_list[0]] ), len( data[language_list[1]] ) )\n",
    "\n",
    "print( \"Training Model\" )\n",
    "n = 0\n",
    "losses = []\n",
    "for epoch in range( 2 ):\n",
    "    for i in range( len( data[language_list[0]] ) ):\n",
    "        label = K.utils.to_categorical( data[language_list[1]][i],\n",
    "                                        output_vocab_size )\n",
    "        loss = model.train_on_batch( [data[language_list[0]][i],\n",
    "                                      data[language_list[1]][i]],\n",
    "                                     label )\n",
    "        losses.append( loss )\n",
    "        n += 1\n",
    "        if n % 3000 == 0:\n",
    "            model.save_weights( \"models/model_weights_\" + str( n ) + \".h5\" )\n",
    "with open( \"losses.txt\", \"w\" ) as f:\n",
    "    for loss in losses:\n",
    "        f.write( str( loss ) + \"\\n\" )\n",
    "\n",
    "data = get_data( \"../data/test/\" )\n",
    "convert_to_index( data, word_to_idx_dict )\n",
    "print( \"Generating sentences\" )\n",
    "translated_sentences = translate_sentences( data[language_list[0]],\n",
    "                                            encoder_model, decoder_model,\n",
    "                                            max_length,\n",
    "                                            word_to_idx_dict[language_list[1]],\n",
    "                                            idx_to_word_dict[language_list[1]] )\n",
    "with open( \"translated_sentence.txt\", \"w\" ) as f:\n",
    "    for sentence in translated_sentences:\n",
    "        f.write( sentence + \"\\n\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
