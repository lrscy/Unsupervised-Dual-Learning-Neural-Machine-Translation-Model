{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import multiprocessing\n",
    "import h5py\n",
    "import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.backend.clear_session()\n",
    "sess = tf.Session( config = tf.ConfigProto( device_count = {'gpu':0} ) )\n",
    "KTF.set_session( sess )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get data from path\n",
    "\n",
    "Args:\n",
    "    path: a string represents corpus path of each language.\n",
    "    language_list: a list of string represents languages.\n",
    "    encoding_list: a list of string represents encoding of each language\n",
    "                   corresponding to language list.\n",
    "    shuffle: a boolean value. True for shuffle.\n",
    "\n",
    "Returns:\n",
    "    lan_data: a dictionary contains sentences of each language.\n",
    "              Its structure is:\n",
    "              \n",
    "              {language A: [[word1, word2, ...], [...], ...],\n",
    "               language B: ...}\n",
    "\"\"\"\n",
    "def get_data( path = \"../Data/\", language_list = [\"chinese\", \"english\"],\n",
    "              encoding_list = [\"UTF-8\", \"UTF-8\"], shuffle = True ):\n",
    "    assert len( lan_list ) != len( encoding_list )\n",
    "    # Just for my convenient in the following\n",
    "    lan_list, enc_list = language_list, encoding_list\n",
    "    \n",
    "    # Read parallel corpus\n",
    "    lan_data = {}\n",
    "    for i in range( len( lan_list ) ):\n",
    "        lan = lan_list[i]\n",
    "        print( \"Reading \" + lan + \" language corpus...\" )\n",
    "        if lan not in lan_data:\n",
    "            lan_data[lan] = []\n",
    "        files = os.listdir( dataPath + lan + \"/\" )\n",
    "        for file in files:\n",
    "            with open( file, \"r\", encoding = enc_list[i] ) as f:\n",
    "                line = f.readline()\n",
    "                while line:\n",
    "                    line = line.strip()\n",
    "                    if len( line ) == 0:\n",
    "                        line = f.readline()\n",
    "                        continue\n",
    "                    words = [\"<S>\"] + line.split() + [\"</S>\"]\n",
    "                    lan_data[lan].append( words )\n",
    "                    line = f.readline()\n",
    "    \n",
    "    if shuffle == True:\n",
    "        print( \"Shuffling...\" )\n",
    "        \n",
    "        # Decide shuffle order\n",
    "        length = data[lan_list[0]].length()\n",
    "        shuf_list = [i for i in range( length )]\n",
    "        random.shuffle( shuf_list )\n",
    "\n",
    "        # Shuffle corpus\n",
    "        for lan in lan_list:\n",
    "            lan_data[lan] = np.array( lan_data[lan] )[shuf_list].tolist()\n",
    "    \n",
    "    return lan_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build dictionary for each language\n",
    "\n",
    "Args:\n",
    "    language_data: a dictionary contains sentences of each language.\n",
    "                   Its structure is:\n",
    "                   \n",
    "                   {language A: [[word1, word2, ...], [...], ...],\n",
    "                    language B: ...}\n",
    "    threshold: a integer represents threshold. If the number of a word\n",
    "               is less than threshold, it will be replaced by <UNK>.\n",
    "\n",
    "Returns:\n",
    "    word_to_idx_dict: a dictionary converts word to index. Its structure is:\n",
    "                      \n",
    "                      {language A: {word A: index A, word B: ..., ...},\n",
    "                       language B: ...}.\n",
    "    idx_to_word_dict: a dictionary converts index to word. Its structure is:\n",
    "                      \n",
    "                      {language A: {index A: word A, index B: ..., ...},\n",
    "                       language B: ...}.\n",
    "\"\"\"\n",
    "def build_dictionary( language_data, threshold = 0 ):\n",
    "    lan_data = language_data\n",
    "    word_to_idx_dict = {}\n",
    "    idx_to_word_dict = {}\n",
    "    for lan, sentences in lan_data.items():\n",
    "        # Generate dictionary for each language\n",
    "        if lan not in word_to_idx_dict:\n",
    "            word_to_idx_dict[lan] = {\"<PAD>\": 0, \"<S>\": 1, \"</S>\": 2, \"<UNK>\": 3}\n",
    "        if lan not in idx_to_word_dict:\n",
    "            idx_to_word_dict[lan] = {0: \"<PAD>\", 1: \"<S>\", 2: \"</S>\", 3: \"<UNK>\"}\n",
    "        \n",
    "        # Count words\n",
    "        word_count = {}\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in word_count:\n",
    "                    word_count[word] = 0\n",
    "                word_count[word] += 1\n",
    "        \n",
    "        # Replace words to <UNK>\n",
    "        for word, count in word_count.items():\n",
    "            if count <= threshold:\n",
    "                word = \"<UNK>\"\n",
    "            if word not in word_to_idx_dict[lan]:\n",
    "                idx = len( word_to_idx_dict[lan] )\n",
    "                word_to_idx_dict[lan][word] = idx\n",
    "                idx_to_word_dict[lan][idx] = word\n",
    "                \n",
    "    return word_to_idx_dict, idx_to_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_seq2seq( output_vocab_size, input_vocab_size,\n",
    "                    hidden_dim = 128, word_vec_dim = 300,\n",
    "                    name = \"baseline\" ):\n",
    "    ### Encoder-Decoder for train ###\n",
    "    \n",
    "    # Encoder\n",
    "    encoder_embedding = Embedding( output_dim = word_vec_dim,\n",
    "                                   input_dim = intput_vocab_size,\n",
    "                                   mask_zero = True,\n",
    "                                   name = name + \"_encoder_embedding\")\n",
    "    encoder = LSTM( hidden_dim, return_state = True,\n",
    "                    name = name + \"_encoder_lstm\" )\n",
    "    encoder_input = Input( shape = ( None, ),\n",
    "                           name = name + \"_encoder_input\" )\n",
    "    \n",
    "    encoder_input_emb   = encoder_embedding( encoder_input )\n",
    "    _, state_h, state_c = encoder( encoder_input_emb )\n",
    "    encoder_state       = [state_h, state_c]\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_embedding = Embedding( output_dim = word_vec_dim,\n",
    "                                   input_dim = output_vocab_size,\n",
    "                                   mask_zero = True,\n",
    "                                   name = name + \"_decoder_embedding\")\n",
    "    decoder = LSTM( hidden_dim, return_state = True, return_sequence = True,\n",
    "                    name = name + \"_decoder_lstm\" )\n",
    "    decoder_dense = Dense( output_vocab_size, activation = \"softmax\",\n",
    "                           name = name + \"_decoder_output\" )\n",
    "    decoder_input = Input( shape = ( None, ),\n",
    "                           name = name + \"_decoder_input\" )\n",
    "    \n",
    "    decoder_input_emb = decoder_embedding( decoder_input )\n",
    "    decoder_output, state_h, state_c = decoder( decoder_input_emb,\n",
    "                                                initial_state = encoder_state )\n",
    "    decoder_state = [state_h, state_c]\n",
    "    decoder_output = decoder_dense( decoder_output )\n",
    "    \n",
    "    # Model\n",
    "    model = Model( inputs = [encoder_input, decoder_input],\n",
    "                   outputs = decoder_outputs,\n",
    "                   name = name )\n",
    "    model.compile( optimizer = 'adam', loss = \"categorical_crossentropy\" )\n",
    "    \n",
    "    ### Encoder-Decoder for generation\n",
    "    \n",
    "    # Encoder Model\n",
    "    encoder_model   = Model( inputs  = encoder_input,\n",
    "                           outputs = encoder_state,\n",
    "                           name = name + \"_encoder\" )\n",
    "    \n",
    "    # Decoder Model\n",
    "    decoder_state_h = Input( shape = ( hidden_dim, ), name = name + \"_state_h\" )\n",
    "    decoder_state_c = Input( shape = ( hidden_dim, ), name = name + \"_state_c\" )\n",
    "    decoder_state_input = [decoder_state_h, decoder_state_c]\n",
    "    decoder_output, state_h, state_c = decoder( decoder_input_emb,\n",
    "                                                initial_state = decoder_state_input )\n",
    "    decoder_state   = [state_h, state_c]\n",
    "    decoder_output  = decoder_dense( decoder_output )\n",
    "    decoder_model   = Model( inputs  = [decoder_input] + decoder_state_input,\n",
    "                             outputs = [decoder_output] + decoder_state,\n",
    "                             name = name + \"_decoder\" )\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
