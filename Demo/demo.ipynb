{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Abstract\n",
    "\n",
    "The demo is a prototype of the project model. Codes and structure here could change in the future.\n",
    "The main point of codes below is to run on local computer and test whether it works on small scale of data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, Lambda, concatenate\n",
    "from keras.models import Model\n",
    "import keras as K\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "K.backend.clear_session()\n",
    "KTF.set_session( tf.Session( config = tf.ConfigProto( device_count = {'gpu':0} ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( \"dogeA\" )\n",
    "# lmA = lm.loadLM( loadPath = \"./lm/chinese\", encoding = \"utf-8\" )\n",
    "# print( \"dogeB\" )\n",
    "# lmB = lm.loadLM( loadPath = \"./lm/english\", encoding = \"utf-8\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Get train data from path\n",
    "\n",
    "Read train data from files for each language and save to a dictionary.\n",
    "\n",
    "Args:\n",
    "    dataPath: file path of train data. Default value is \"../../Data/train/\".\n",
    "    langList: language list. Indicating which language data will be included in train data.\n",
    "              Default value is [\"Chinese\", \"English\"].\n",
    "    encoding: encoding of each file in train directory.\n",
    "    ratio: propotion of train data, others will be treat as dev data. Default value is 0.98.\n",
    "    sort: boolean value. If shuffle equals True, all data will be sorted according to their\n",
    "          length from short to long. Otherwise, train sentences will be shuffled at the end.\n",
    "          Default value is True.\n",
    "\n",
    "Returns:\n",
    "    trainData: a dictionary of train data sentences of each language. Its structure is:\n",
    "    \n",
    "               {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \n",
    "    devData: a dictionary of dev data sentences of each language. Its structure is:\n",
    "    \n",
    "               {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "\"\"\"\n",
    "def getTrainData( dataPath = \"../Data/train/\", lanList = [\"chinese\", \"english\"],\n",
    "                  encoding = \"UTF-8\", ratio = 0.98, sort = True ):\n",
    "    trainData = {}\n",
    "    devData   = {}\n",
    "    for lan in lanList:\n",
    "        print( lan )\n",
    "        if lan not in trainData:\n",
    "            trainData[lan] = []\n",
    "        if lan not in devData:\n",
    "            devData[lan] = []\n",
    "        files = os.listdir( dataPath + lan + \"/\" )\n",
    "        data = []\n",
    "        for file in files:\n",
    "            with open( dataPath + lan + \"/\" + file, encoding = encoding ) as f:\n",
    "                line = f.readline()\n",
    "                while line:\n",
    "                    wordList = line.split()\n",
    "                    data.append( [\"<S>\"] + wordList + [\"</S>\"] )\n",
    "                    line = f.readline()\n",
    "        # suffle here is to make sure that all data are random distributed\n",
    "        random.shuffle( data )\n",
    "        noOfSentences = len( data )\n",
    "        noOfTrainData = int( noOfSentences * ratio )\n",
    "        devData[lan]   = data[noOfTrainData:]\n",
    "        trainData[lan] = data[:noOfTrainData]\n",
    "        if sort == True:\n",
    "            trainData[lan].sort()\n",
    "            devData[lan].sort()\n",
    "    return trainData, devData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Generate dictionary and preprocess setences for each language\n",
    "\n",
    "Generate dictionary for each language and convert word to corresponding index.\n",
    "Here we set two dictionaries to speed up the whole program.\n",
    "\n",
    "Args:\n",
    "    data: a dictionary contains sentences of each language.  Its structure is:\n",
    "    \n",
    "          {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \n",
    "    threshold: a word will be replace with <UNK> if frequency of a word is\n",
    "               less than threshold. If the value is less than 1, it means\n",
    "               no need to replace any word to <UNK>. Default value is 0.\n",
    "\n",
    "Returns:\n",
    "    wordNumDict: a dictionary which can convert words to index in each language.\n",
    "                 Its structure is:\n",
    "                 \n",
    "                 {language A: {word A: index, word B: ..., ...}, language B: ..., ...}.\n",
    "\n",
    "    numWordDict: a dictionary which can convert index to word in each language.\n",
    "                 Its structure is:\n",
    "                 \n",
    "                 {language A: {word A: index, word B: ..., ...}, language B: ..., ...}.\n",
    "\"\"\"\n",
    "def generateDict( data, threshold = 0 ):\n",
    "    wordNumDict = {}\n",
    "    numWordDict = {}\n",
    "    for lan, sentences in data.items():\n",
    "        wordCount = {}\n",
    "        if lan not in wordNumDict:\n",
    "            # Add special word to dictionary\n",
    "            wordNumDict[lan] = {\"<PAD>\": 0, \"<S>\": 1, \"</S>\": 2, \"<UNK>\": 3}\n",
    "        if lan not in numWordDict:\n",
    "            # Add special word to dictionary\n",
    "            numWordDict[lan] = {0: \"<PAD>\", 1: \"<S>\", 2: \"</S>\", 3: \"<UNK>\"}\n",
    "        \n",
    "        # Count word frequency\n",
    "        for sentence in sentences:\n",
    "            for i in range( len( sentence ) ):\n",
    "                word = sentence[i]\n",
    "                if word not in wordCount:\n",
    "                    wordCount[word] = 0\n",
    "                wordCount[word] += 1\n",
    "        print( len( wordCount ) )\n",
    "        \n",
    "        # Find and replace with <UNK>\n",
    "#         for k, v in wordCount.items():\n",
    "#             if v < threshold:\n",
    "#                 k = \"<UNK>\"\n",
    "#             if k not in wordNumDict[lan]:\n",
    "#                 number = len( wordNumDict[lan] )\n",
    "#                 wordNumDict[lan][k] = number\n",
    "#                 numWordDict[lan][number] = k\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for i in range( len( sentence ) ):\n",
    "                word = sentence[i]\n",
    "                if wordCount[word] < threshold:\n",
    "                    word = \"<UNK>\"\n",
    "                if word not in wordNumDict[lan]:\n",
    "                    number = len( wordNumDict[lan] )\n",
    "                    wordNumDict[lan][word] = number\n",
    "                    numWordDict[lan][number] = word\n",
    "                sentence[i] = wordNumDict[lan][word]\n",
    "    return wordNumDict, numWordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chinese\n",
      "english\n",
      "37077\n",
      "27897\n",
      "37079 37079\n",
      "1 2 3 0\n",
      "27899 27899\n",
      "1 2 3 0\n",
      "[1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 4, 20, 21, 22, 23, 5, 24, 4, 25, 4, 8, 26, 27, 28, 2]\n"
     ]
    }
   ],
   "source": [
    "trainData, devData = getTrainData( \"../Data/test/\")\n",
    "wordNumDict, numWordDict = generateDict( trainData )\n",
    "print( len( wordNumDict[\"chinese\"] ), len( numWordDict[\"chinese\"] ) )\n",
    "print( wordNumDict[\"chinese\"][\"<S>\"],\n",
    "       wordNumDict[\"chinese\"][\"</S>\"],\n",
    "       wordNumDict[\"chinese\"][\"<UNK>\"],\n",
    "       wordNumDict[\"chinese\"][\"<PAD>\"],\n",
    "     )\n",
    "print( len( wordNumDict[\"english\"] ), len( numWordDict[\"english\"] ) )\n",
    "print( wordNumDict[\"english\"][\"<S>\"],\n",
    "       wordNumDict[\"english\"][\"</S>\"],\n",
    "       wordNumDict[\"english\"][\"<UNK>\"],\n",
    "       wordNumDict[\"english\"][\"<PAD>\"],\n",
    "     )\n",
    "print( trainData[\"chinese\"][0] )\n",
    "# model.fit( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss( y_true, y_pred ):\n",
    "    y_rev = K.backend.reverse( y_pred, axes = -1 )\n",
    "    loss = K.backend.sum( y_pred * y_rev / 2, axis = -1, keepdims = True )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dual Neural Machine Translation Model\n",
    "\n",
    "The structure of the model is:\n",
    "\n",
    "Input A ---> RNN1 ---> Output B ---> RNN2 ---> Output A\n",
    "                          |                       |\n",
    "                          V                       V\n",
    "                        LM B                    LM A\n",
    "\n",
    "Args:\n",
    "    langA_dim:  dimension of language A word vector.\n",
    "    langB_dim:  dimension of language B word vector.\n",
    "    hidden_dim: dimension of hidden state vector.\n",
    "    lmA: language model of language A.\n",
    "    lmB: language model of language B.\n",
    "\n",
    "Returns:\n",
    "    model: the whole model of Dual Neural Machine Translation Model.\n",
    "\"\"\"\n",
    "def dualNMTModel( langA_vocab_size = 100000, langB_vocab_size = 100000,\n",
    "                  langA_dim = 1024, langB_dim = 1024, word_vec_dim = 256, hidden_dim = 256,\n",
    "                  name = \"demo\" ):\n",
    "    # first part of translation model from language A to language B\n",
    "    embedding_A2B = Embedding( output_dim = word_vec_dim, input_dim = langA_vocab_size,\n",
    "                               name = name + \"_embedding_A2B\", mask_zero = True )\n",
    "    to_label = Lambda( lambda x: K.backend.argmax( x, axis = 2 ) )\n",
    "    avg_emb  = Lambda( lambda x: tf.math.reduce_mean( x, axis = 1 ) )\n",
    "    rev_emb  = Lambda( lambda x: K.backend.reverse( x, axes = 1 ) )\n",
    "    # Encoder\n",
    "    encoder_input_A2B     = Input( shape = ( None, ), name = name + \"_encoder_input_A2B\" )\n",
    "    # change when using pre-trained embedding trainable= False\n",
    "    encoder_A2B           = LSTM( hidden_dim, return_state = True )\n",
    "    encoder_input_emb_A2B = embedding_A2B( encoder_input_A2B )\n",
    "    _, state_h, state_c   = encoder_A2B( encoder_input_emb_A2B )\n",
    "    state_encoder_A2B     = [state_h, state_c]\n",
    "    # Decoder\n",
    "    decoder_A2B = LSTM( hidden_dim, return_sequences = True )\n",
    "\n",
    "    decoder_input_A2B     = Input( shape = ( None, ), name = name + \"_decoder_input_A2B\" )\n",
    "    decoder_input_emb_A2B = embedding_A2B( decoder_input_A2B )\n",
    "    decoder_outputs_A2B   = decoder_A2B( decoder_input_emb_A2B, initial_state = state_encoder_A2B )\n",
    "    decoder_dense_A2B     = Dense( langB_vocab_size, activation = \"softmax\", name = name + \"_decoder_output_A2B\" )\n",
    "    decoder_outputs_A2B   = decoder_dense_A2B( decoder_outputs_A2B )\n",
    "    \n",
    "    # language model <- langB_output\n",
    "    # lossB = Lambda( perplexity )( langB_output )\n",
    "    \n",
    "    # second part of translation model from another language to original language\n",
    "    # first part of translation model from language A to language B\n",
    "    embedding_B2A = Embedding( output_dim = word_vec_dim, input_dim = langB_vocab_size,\n",
    "                               name = name + \"_embedding_B2A\", mask_zero = True )\n",
    "    # Encoder\n",
    "    encoder_input_B2A     = to_label( decoder_outputs_A2B )\n",
    "    # change when using pre-trained embedding trainable= False\n",
    "    encoder_B2A           = LSTM( hidden_dim, return_state = True )\n",
    "    encoder_input_emb_B2A = embedding_B2A( encoder_input_B2A )\n",
    "    _, state_h, state_c   = encoder_B2A( encoder_input_emb_B2A )\n",
    "    state_encoder_B2A     = [state_h, state_c]\n",
    "    # Decoder\n",
    "    decoder_B2A = LSTM( hidden_dim, return_sequences = True )\n",
    "\n",
    "    decoder_input_B2A     = Input( shape = ( None, ), name = name + \"_decoder_input_B2A\" )\n",
    "    decoder_input_emb_B2A = embedding_B2A( decoder_input_B2A )\n",
    "    decoder_outputs_B2A   = decoder_B2A( decoder_input_emb_B2A, initial_state = state_encoder_B2A )\n",
    "    decoder_dense_B2A     = Dense( langA_vocab_size, activation = \"softmax\", name = name + \"_decoder_output_B2A\" )\n",
    "    decoder_outputs_B2A   = decoder_dense_B2A( decoder_outputs_B2A )\n",
    "    \n",
    "    avg_input_emb  = avg_emb( encoder_input_emb_A2B )\n",
    "    decoder_outputs_label = to_label( decoder_outputs_B2A )\n",
    "    decoder_outputs_emb   = embedding_A2B( decoder_outputs_label )\n",
    "    avg_output_emb = avg_emb( decoder_outputs_emb )\n",
    "    avg_output_emb = rev_emb( avg_output_emb )\n",
    "    \n",
    "    output = concatenate( [avg_input_emb, avg_output_emb], axis = 1 )\n",
    "    \n",
    "    # language model <- langA_output\n",
    "    # lossA = Lambda( perplexity )( langA_output )\n",
    "    \n",
    "    # Build model\n",
    "    model = Model( inputs = [encoder_input_A2B, decoder_input_A2B, decoder_input_B2A],\n",
    "                   outputs = output ) #[decoder_outputs_B2A, decoder_outputs_A2B]\n",
    "    model.compile( optimizer = 'adam', loss = loss ) #, loss_weights = [0.5, 1.]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogeLM\n"
     ]
    }
   ],
   "source": [
    "print( \"dogeLM\" )\n",
    "model = dualNMTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "demo_encoder_input_A2B (InputLa (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "demo_embedding_A2B (Embedding)  (None, None, 256)    25600000    demo_encoder_input_A2B[0][0]     \n",
      "                                                                 demo_decoder_input_A2B[0][0]     \n",
      "                                                                 lambda_1[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "demo_decoder_input_A2B (InputLa (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 525312      demo_embedding_A2B[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, None, 256)    525312      demo_embedding_A2B[1][0]         \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "demo_decoder_output_A2B (Dense) (None, None, 100000) 25700000    lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None)         0           demo_decoder_output_A2B[0][0]    \n",
      "                                                                 demo_decoder_output_B2A[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "demo_decoder_input_B2A (InputLa (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "demo_embedding_B2A (Embedding)  (None, None, 256)    25600000    lambda_1[0][0]                   \n",
      "                                                                 demo_decoder_input_B2A[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 256), (None, 525312      demo_embedding_B2A[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, None, 256)    525312      demo_embedding_B2A[1][0]         \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "demo_decoder_output_B2A (Dense) (None, None, 100000) 25700000    lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 256)          0           demo_embedding_A2B[0][0]         \n",
      "                                                                 demo_embedding_A2B[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 256)          0           lambda_2[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 104,701,248\n",
      "Trainable params: 104,701,248\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 3 array(s), but instead got the following list of 2 arrays: [array([[list([1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 4, 20, 21, 22, 23, 5, 24, 4, 25, 4, 8, 26, 27, 28, 2])],\n       [list([1, 4, 5, 29, 30, 31, 11, 32, 33, 34, 35, 5, 36, 37, 3...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-52a701c93d28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"chinese\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 3 array(s), but instead got the following list of 2 arrays: [array([[list([1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 4, 20, 21, 22, 23, 5, 24, 4, 25, 4, 8, 26, 27, 28, 2])],\n       [list([1, 4, 5, 29, 30, 31, 11, 32, 33, 34, 35, 5, 36, 37, 3..."
     ]
    }
   ],
   "source": [
    "model.fit( [trainData[\"chinese\"], []], [] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Simple Seqence to Sequence Implementation\n",
    "\n",
    "A simple implementation of Sequence to Sequence model. It works as baseline\n",
    "\n",
    "Args:\n",
    "    input_dim:  dimension of input word vector.\n",
    "    output_dim: dimension of output word vector.\n",
    "    hidden_dim: dimension of hidden states vector.\n",
    "    output_vocab_size: size of output language vocabulary size.\n",
    "    input_vocab_size:  size of input  language vocabulary size.\n",
    "    word_vec_dim: dimension of word-vector.\n",
    "    name: name of the model.\n",
    "\n",
    "Returns:\n",
    "    model: the whole model of simple Seq2Seq model.\n",
    "\n",
    "\"\"\"\n",
    "def simpleSeq2Seq( output_vocab_size, input_vocab_size, hidden_dim = 256,\n",
    "                   word_vec_dim = 512, name = \"demo\" ):\n",
    "    embedding = Embedding( output_dim = word_vec_dim, input_dim = input_vocab_size,\n",
    "                           name = name + \"_embedding\" ) # , mask_zero = True\n",
    "    # Encoder\n",
    "    encoder_input     = Input( shape = ( None, ), name = name + \"_encoder_input\" )\n",
    "    # change when using pre-trained embedding trainable= False\n",
    "    encoder           = LSTM( hidden_dim, return_state = True )\n",
    "    encoder_input_emb = embedding( encoder_input )\n",
    "    _, state_h, state_c = encoder( encoder_input_emb )\n",
    "    state_encoder     = [state_h, state_c]\n",
    "    # Decoder\n",
    "    decoder = LSTM( hidden_dim, return_sequences = True )\n",
    "\n",
    "    decoder_input     = Input( shape = ( None, ), name = name + \"_decoder_input\" )\n",
    "    decoder_input_emb = embedding( decoder_input )\n",
    "    decoder_outputs   = decoder( decoder_input_emb, initial_state = state_encoder )\n",
    "    decoder_dense     = Dense( output_vocab_size, activation = \"softmax\", name = name + \"_decoder_output\" )\n",
    "    decoder_outputs   = decoder_dense( decoder_outputs )\n",
    "\n",
    "    # Build model\n",
    "    model = Model( inputs = [encoder_input, decoder_input], outputs = decoder_outputs, name = name )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"dogesimple\" )\n",
    "model = simpleSeq2Seq( output_vocab_size = 24321714, input_vocab_size = 20735725, name = \"demo\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
