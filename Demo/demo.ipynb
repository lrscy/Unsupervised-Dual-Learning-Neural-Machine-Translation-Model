{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Abstract\\n\\nThe demo is a prototype of the project model. Codes and structure here could change in the future.\\nThe main point of codes below is to run on local computer and test whether it works on small scale of data.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Abstract\n",
    "\n",
    "The demo is a prototype of the project model. Codes and structure here could change in the future.\n",
    "The main point of codes below is to run on local computer and test whether it works on small scale of data.\n",
    "\n",
    "Now:\n",
    "    Prove that original model is inaccessible. Start to implemente original RNN-LSTM and treat it as baseline.\n",
    "\n",
    "TODO:\n",
    "    1. Try new way to approach the unsupervised method.\n",
    "    2. Try to understand how to control gradient in Tensorflow.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, Lambda, concatenate, Multiply\n",
    "from keras.models import Model\n",
    "import keras as K\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "K.backend.clear_session()\n",
    "KTF.set_session( tf.Session( config = tf.ConfigProto( device_count = {'gpu':0} ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( \"dogeA\" )\n",
    "# lmA = lm.loadLM( loadPath = \"./lm/chinese\", encoding = \"utf-8\" )\n",
    "# print( \"dogeB\" )\n",
    "# lmB = lm.loadLM( loadPath = \"./lm/english\", encoding = \"utf-8\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Get train data from path\n",
    "\n",
    "Read train data from files for each language and save to a dictionary.\n",
    "\n",
    "Args:\n",
    "    dataPath: file path of train data. Default value is \"../../Data/train/\".\n",
    "    langList: language list. Indicating which language data will be included in train data.\n",
    "              Default value is [\"Chinese\", \"English\"].\n",
    "    encoding: encoding of each file in train directory.\n",
    "    ratio: propotion of train data, others will be treat as dev data. Default value is 0.98.\n",
    "    sort: boolean value. If shuffle equals True, all data will be sorted according to their\n",
    "          length from short to long. Otherwise, train sentences will be shuffled at the end.\n",
    "          Default value is True.\n",
    "\n",
    "Returns:\n",
    "    trainData: a dictionary of train data sentences of each language. Its structure is:\n",
    "    \n",
    "               {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \n",
    "    devData: a dictionary of dev data sentences of each language. Its structure is:\n",
    "    \n",
    "               {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "\"\"\"\n",
    "def getTrainData( dataPath = \"../Data/train/\", lanList = [\"chinese\", \"english\"],\n",
    "                  encoding = \"UTF-8\", ratio = 0.98, sort = True ):\n",
    "    trainData = {}\n",
    "    devData   = {}\n",
    "    for lan in lanList:\n",
    "        print( lan )\n",
    "        if lan not in trainData:\n",
    "            trainData[lan] = []\n",
    "        if lan not in devData:\n",
    "            devData[lan] = []\n",
    "        files = os.listdir( dataPath + lan + \"/\" )\n",
    "        data = []\n",
    "        for file in files:\n",
    "            with open( dataPath + lan + \"/\" + file, encoding = encoding ) as f:\n",
    "                line = f.readline()\n",
    "                while line:\n",
    "                    wordList = line.split()\n",
    "                    data.append( [\"<S>\"] + wordList + [\"</S>\"] )\n",
    "                    line = f.readline()\n",
    "        # suffle here is to make sure that all data are random distributed\n",
    "        random.shuffle( data )\n",
    "        noOfSentences = len( data )\n",
    "        noOfTrainData = int( noOfSentences * ratio )\n",
    "        devData[lan]   = data[noOfTrainData:]\n",
    "        trainData[lan] = data[:noOfTrainData]\n",
    "        if sort == True:\n",
    "            trainData[lan].sort( key = lambda x: len( x ) )\n",
    "            devData[lan].sort( key = lambda x: len( x ) )\n",
    "    return trainData, devData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Generate dictionary and preprocess setences for each language\n",
    "\n",
    "Generate dictionary for each language and convert word to corresponding index.\n",
    "Here we set two dictionaries to speed up the whole program.\n",
    "\n",
    "Args:\n",
    "    data: a dictionary contains sentences of each language.  Its structure is:\n",
    "    \n",
    "          {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \n",
    "    threshold: a word will be replace with <UNK> if frequency of a word is\n",
    "               less than threshold. If the value is less than 1, it means\n",
    "               no need to replace any word to <UNK>. Default value is 0.\n",
    "\n",
    "Returns:\n",
    "    wordNumDict: a dictionary which can convert words to index in each language.\n",
    "                 Its structure is:\n",
    "                 \n",
    "                 {language A: {word A: index, word B: ..., ...}, language B: ..., ...}.\n",
    "    \n",
    "    numWordDict: a dictionary which can convert index to word in each language.\n",
    "                 Its structure is:\n",
    "                 \n",
    "                 {language A: {word A: index, word B: ..., ...}, language B: ..., ...}.\n",
    "\"\"\"\n",
    "def generateDict( data, threshold = 0 ):\n",
    "    wordNumDict = {}\n",
    "    numWordDict = {}\n",
    "    for lan, sentences in data.items():\n",
    "        wordCount = {}\n",
    "        if lan not in wordNumDict:\n",
    "            # Add special word to dictionary\n",
    "            wordNumDict[lan] = {\"<PAD>\": 0, \"<S>\": 1, \"</S>\": 2, \"<UNK>\": 3}\n",
    "        if lan not in numWordDict:\n",
    "            # Add special word to dictionary\n",
    "            numWordDict[lan] = {0: \"<PAD>\", 1: \"<S>\", 2: \"</S>\", 3: \"<UNK>\"}\n",
    "        \n",
    "        # Count word frequency\n",
    "        for sentence in sentences:\n",
    "            for i in range( len( sentence ) ):\n",
    "                word = sentence[i]\n",
    "                if word not in wordCount:\n",
    "                    wordCount[word] = 0\n",
    "                wordCount[word] += 1\n",
    "        \n",
    "        # Find and replace with <UNK>\n",
    "        for sentence in sentences:\n",
    "            for i in range( len( sentence ) ):\n",
    "                word = sentence[i]\n",
    "                if wordCount[word] < threshold:\n",
    "                    word = \"<UNK>\"\n",
    "                if word not in wordNumDict[lan]:\n",
    "                    number = len( wordNumDict[lan] )\n",
    "                    wordNumDict[lan][word] = number\n",
    "                    numWordDict[lan][number] = word\n",
    "                sentence[i] = wordNumDict[lan][word]\n",
    "    return wordNumDict, numWordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Number to One-hot\n",
    "\n",
    "Only convert sentences which length are small than 30.\n",
    "\n",
    "Args:\n",
    "    data: a dictionary contains sentences of each language.  Its structure is:\n",
    "    \n",
    "          {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \n",
    "    wordNumDict: a dictionary which can convert words to index in each language.\n",
    "                 Its structure is:\n",
    "                 \n",
    "                 {language A: {word A: index, word B: ..., ...}, language B: ..., ...}.\n",
    "\n",
    "Returns:\n",
    "    ndata: \n",
    "    td:\n",
    "\"\"\"\n",
    "def toCategory( data, wordNumDict, left, right ):\n",
    "    n = right - left\n",
    "    maxlch = 0\n",
    "    maxlen = 0\n",
    "    for i in range( left, right ):\n",
    "        if len( data[\"chinese\"][i] ) > 20:\n",
    "            continue\n",
    "        n += 1\n",
    "        maxlch = np.max( [maxlch, len( data[\"chinese\"][i] )] )\n",
    "        maxlen = np.max( [maxlen, len( data[\"english\"][i] )] )\n",
    "#     print( n, maxlen, len( wordNumDict[\"english\"] ) )\n",
    "#     print( n * maxlen * len( wordNumDict[\"english\"] ) )\n",
    "    zh = np.zeros( ( n, maxlch ) )\n",
    "    en = np.zeros( ( n, maxlen ) )\n",
    "    td = np.zeros( ( n, maxlen, len( wordNumDict[\"english\"] ) ) )\n",
    "    for i in range( left, right ):\n",
    "        if len( data[\"chinese\"][i] ) > 20:\n",
    "            continue\n",
    "        for j in range( len( data[\"chinese\"][i] ) ):\n",
    "            zh[i - left, j] = data[\"english\"][i][j]\n",
    "        for j in range( len( data[\"english\"][i] ) ):\n",
    "            en[i - left, j] = data[\"english\"][i][j]\n",
    "            if j:\n",
    "                w = data[\"english\"][i][j]\n",
    "                td[i - left, j - 1, w] = 1\n",
    "    ndata = {}\n",
    "    ndata[\"chinese\"] = zh\n",
    "    ndata[\"english\"] = en\n",
    "    return ndata, td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Simple Seqence to Sequence Implementation\n",
    "\n",
    "A simple implementation of Sequence to Sequence model. It works as baseline\n",
    "\n",
    "Args:\n",
    "    input_dim:  dimension of input word vector.\n",
    "    output_dim: dimension of output word vector.\n",
    "    hidden_dim: dimension of hidden states vector.\n",
    "    output_vocab_size: size of output language vocabulary size.\n",
    "    input_vocab_size:  size of input  language vocabulary size.\n",
    "    word_vec_dim: dimension of word-vector.\n",
    "    name: name of the model.\n",
    "\n",
    "Returns:\n",
    "    model: the whole model of simple Seq2Seq model.\n",
    "\n",
    "\"\"\"\n",
    "def simpleSeq2Seq( output_vocab_size, input_vocab_size, hidden_dim = 256,\n",
    "                   word_vec_dim = 512, name = \"demo\" ):\n",
    "    embedding_encoder  = Embedding( output_dim = word_vec_dim, input_dim = input_vocab_size,\n",
    "                                 name = name + \"_encoder_embedding\", mask_zero = True ) # \n",
    "    embedding_decoder = Embedding( output_dim = word_vec_dim, input_dim = output_vocab_size,\n",
    "                                 name = name + \"_decoder_embedding\", mask_zero = True ) # \n",
    "    # Encoder\n",
    "    encoder_input     = Input( shape = ( None, ), name = name + \"_encoder_input\" )\n",
    "    # change when using pre-trained embedding trainable= False\n",
    "    encoder           = LSTM( hidden_dim, return_state = True )\n",
    "    encoder_input_emb = embedding_encoder( encoder_input )\n",
    "    _, state_h, state_c = encoder( encoder_input_emb )\n",
    "    state_encoder     = [state_h, state_c]\n",
    "    # Decoder\n",
    "    decoder = LSTM( hidden_dim, return_sequences = True )\n",
    "\n",
    "    decoder_input     = Input( shape = ( None, ), name = name + \"_decoder_input\" )\n",
    "    decoder_input_emb = embedding_decoder( decoder_input )\n",
    "    decoder_outputs   = decoder( decoder_input_emb, initial_state = state_encoder )\n",
    "    decoder_dense     = Dense( output_vocab_size, activation = \"softmax\", name = name + \"_decoder_output\" )\n",
    "    decoder_outputs   = decoder_dense( decoder_outputs )\n",
    "\n",
    "    # Build model\n",
    "    model = Model( inputs = [encoder_input, decoder_input], outputs = decoder_outputs, name = name )\n",
    "    model.compile( optimizer = 'adam', loss = \"categorical_crossentropy\" )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chinese\n",
      "english\n",
      "88391 60557\n"
     ]
    }
   ],
   "source": [
    "trainData, devData = getTrainData( \"../Data/train/\" )\n",
    "wordNumDict, numWordDict = generateDict( trainData, threshold = 5 )\n",
    "ivs = len( wordNumDict[\"chinese\"] )\n",
    "ovs = len( wordNumDict[\"english\"] )\n",
    "print( ivs, ovs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "demo_encoder_input (InputLayer) (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "demo_decoder_input (InputLayer) (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "demo_encoder_embedding (Embeddi (None, None, 512)    45256192    demo_encoder_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "demo_decoder_embedding (Embeddi (None, None, 512)    31005184    demo_decoder_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 787456      demo_encoder_embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, None, 256)    787456      demo_decoder_embedding[0][0]     \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "demo_decoder_output (Dense)     (None, None, 60557)  15563149    lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 93,399,437\n",
      "Trainable params: 93,399,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-17f815378774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"chinese\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnewTrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoCategory\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordNumDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnewTrainData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"chinese\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewTrainData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2653\u001b[0m                 array_vals.append(\n\u001b[1;32m   2654\u001b[0m                     np.asarray(value,\n\u001b[0;32m-> 2655\u001b[0;31m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[1;32m   2656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = simpleSeq2Seq( output_vocab_size = ovs, input_vocab_size = ivs, name = \"demo\" )\n",
    "model.summary()\n",
    "# model = simpleSeq2Seq( output_vocab_size = 24321714, input_vocab_size = 20735725, name = \"demo\" )\n",
    "\n",
    "for i in range( 0, len( trainData[\"chinese\"] ) + 32, 32 ):\n",
    "    newTrainData, td = toCategory( trainData, wordNumDict, i, i + 32 )\n",
    "    model.train_on_batch( [newTrainData[\"chinese\"], newTrainData[\"english\"]], td )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss( y_true, y_pred ):\n",
    "    y1 = y_pred[:, :256]\n",
    "    y2 = y_pred[:, 256:]\n",
    "    return y1 * y2\n",
    "#     y_rev = K.backend.reverse( y_pred, axes = -1 )\n",
    "#     loss = K.backend.sum( y_pred * y_rev / 2, axis = -1, keepdims = True )\n",
    "#     return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dual Neural Machine Translation Model\n",
    "\n",
    "The structure of the model is:\n",
    "\n",
    "Input A ---> RNN1 ---> Output B ---> RNN2 ---> Output A\n",
    "                          |                       |\n",
    "                          V                       V\n",
    "                        LM B                    LM A\n",
    "\n",
    "Args:\n",
    "    langA_dim:  dimension of language A word vector.\n",
    "    langB_dim:  dimension of language B word vector.\n",
    "    hidden_dim: dimension of hidden state vector.\n",
    "    lmA: language model of language A.\n",
    "    lmB: language model of language B.\n",
    "\n",
    "Returns:\n",
    "    model: the whole model of Dual Neural Machine Translation Model.\n",
    "\"\"\"\n",
    "def dualNMTModel( langA_vocab_size = 100000, langB_vocab_size = 100000,\n",
    "                  langA_dim = 1024, langB_dim = 1024, word_vec_dim = 256, hidden_dim = 256,\n",
    "                  name = \"demo\" ):\n",
    "    # first part of translation model from language A to language B\n",
    "    embedding_A2B = Embedding( output_dim = word_vec_dim, input_dim = langA_vocab_size,\n",
    "                               name = name + \"_embedding_A2B\", mask_zero = True )\n",
    "    to_label = Lambda( lambda x: K.backend.argmax( x, axis = 2 ) )\n",
    "    avg_emb  = Lambda( lambda x: tf.math.reduce_mean( x, axis = 1 ) )\n",
    "    rev_emb  = Lambda( lambda x: K.backend.reverse( x, axes = 1 ) )\n",
    "    cos_sim  = Lambda( lambda x, y: x * y )\n",
    "    # Encoder\n",
    "    encoder_input_A2B     = Input( shape = ( None, ), name = name + \"_encoder_input_A2B\" )\n",
    "    # change when using pre-trained embedding trainable= False\n",
    "    encoder_A2B           = LSTM( hidden_dim, return_state = True )\n",
    "    encoder_input_emb_A2B = embedding_A2B( encoder_input_A2B )\n",
    "    _, state_h, state_c   = encoder_A2B( encoder_input_emb_A2B )\n",
    "    state_encoder_A2B     = [state_h, state_c]\n",
    "    # Decoder\n",
    "    decoder_A2B = LSTM( hidden_dim, return_sequences = True )\n",
    "\n",
    "    decoder_input_A2B     = Input( shape = ( None, ), name = name + \"_decoder_input_A2B\" )\n",
    "    decoder_input_emb_A2B = embedding_A2B( decoder_input_A2B )\n",
    "    decoder_outputs_A2B   = decoder_A2B( decoder_input_emb_A2B, initial_state = state_encoder_A2B )\n",
    "    decoder_dense_A2B     = Dense( langB_vocab_size, activation = \"softmax\", name = name + \"_decoder_output_A2B\" )\n",
    "    decoder_outputs_A2B   = decoder_dense_A2B( decoder_outputs_A2B )\n",
    "    \n",
    "    # language model <- langB_output\n",
    "    # lossB = Lambda( perplexity )( langB_output )\n",
    "    \n",
    "    # second part of translation model from another language to original language\n",
    "    # first part of translation model from language A to language B\n",
    "    embedding_B2A = Embedding( output_dim = word_vec_dim, input_dim = langB_vocab_size,\n",
    "                               name = name + \"_embedding_B2A\", mask_zero = True )\n",
    "    # Encoder\n",
    "    encoder_input_B2A     = to_label( decoder_outputs_A2B )\n",
    "    # change when using pre-trained embedding trainable= False\n",
    "    encoder_B2A           = LSTM( hidden_dim, return_state = True )\n",
    "    encoder_input_emb_B2A = embedding_B2A( encoder_input_B2A )\n",
    "#     _, state_h, state_c   = encoder_B2A( decoder_outputs_A2B )\n",
    "    _, state_h, state_c   = encoder_B2A( encoder_input_emb_B2A )\n",
    "    state_encoder_B2A     = [state_h, state_c]\n",
    "    # Decoder\n",
    "    decoder_B2A = LSTM( hidden_dim, return_sequences = True )\n",
    "\n",
    "    decoder_input_B2A     = Input( shape = ( None, ), name = name + \"_decoder_input_B2A\" )\n",
    "    decoder_input_emb_B2A = embedding_B2A( decoder_input_B2A )\n",
    "    decoder_outputs_B2A   = decoder_B2A( decoder_input_emb_B2A, initial_state = state_encoder_B2A )\n",
    "    decoder_dense_B2A     = Dense( langA_vocab_size, activation = \"softmax\", name = name + \"_decoder_output_B2A\" )\n",
    "    decoder_outputs_B2A   = decoder_dense_B2A( decoder_outputs_B2A )\n",
    "    \n",
    "    avg_input_emb  = avg_emb( encoder_input_emb_A2B )\n",
    "    decoder_outputs_label = to_label( decoder_outputs_B2A )\n",
    "    decoder_outputs_emb   = embedding_A2B( decoder_outputs_label )\n",
    "    avg_output_emb = avg_emb( decoder_outputs_emb )\n",
    "#     avg_output_emb = rev_emb( avg_output_emb )\n",
    "    \n",
    "    output = Multiply()( [avg_input_emb, avg_output_emb] )\n",
    "    \n",
    "#     output = concatenate( [avg_input_emb, avg_output_emb], axis = 1 )\n",
    "    \n",
    "    # language model <- langA_output\n",
    "    # lossA = Lambda( perplexity )( langA_output )\n",
    "    \n",
    "    # Build model\n",
    "    model = Model( inputs = [encoder_input_A2B, decoder_input_A2B, decoder_input_B2A],\n",
    "                   outputs = output ) #[decoder_outputs_B2A, decoder_outputs_A2B]\n",
    "    model.compile( optimizer = 'adam', loss = lambda y_true, y_pred: y_pred ) #, loss_weights = [0.5, 1.]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogeLM\n"
     ]
    }
   ],
   "source": [
    "print( \"dogeLM\" )\n",
    "model = dualNMTModel( langA_vocab_size = 37100, langB_vocab_size = 27869 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "demo_encoder_input_A2B (InputLa (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "demo_embedding_A2B (Embedding)  (None, None, 256)    9497600     demo_encoder_input_A2B[0][0]     \n",
      "                                                                 demo_decoder_input_A2B[0][0]     \n",
      "                                                                 lambda_9[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "demo_decoder_input_A2B (InputLa (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  [(None, 256), (None, 525312      demo_embedding_A2B[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  (None, None, 256)    525312      demo_embedding_A2B[1][0]         \n",
      "                                                                 lstm_13[0][1]                    \n",
      "                                                                 lstm_13[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "demo_decoder_output_A2B (Dense) (None, None, 27869)  7162333     lstm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, None)         0           demo_decoder_output_A2B[0][0]    \n",
      "                                                                 demo_decoder_output_B2A[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "demo_decoder_input_B2A (InputLa (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "demo_embedding_B2A (Embedding)  (None, None, 256)    7134464     lambda_9[0][0]                   \n",
      "                                                                 demo_decoder_input_B2A[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  [(None, 256), (None, 525312      demo_embedding_B2A[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  (None, None, 256)    525312      demo_embedding_B2A[1][0]         \n",
      "                                                                 lstm_15[0][1]                    \n",
      "                                                                 lstm_15[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "demo_decoder_output_B2A (Dense) (None, None, 37100)  9534700     lstm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 256)          0           demo_embedding_A2B[0][0]         \n",
      "                                                                 demo_embedding_A2B[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 256)          0           lambda_10[0][0]                  \n",
      "                                                                 lambda_10[1][0]                  \n",
      "==================================================================================================\n",
      "Total params: 35,430,345\n",
      "Trainable params: 35,430,345\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, devData = getTrainData( \"../Data/test/\" )\n",
    "wordNumDict, numWordDict = generateDict( trainData )\n",
    "print( len( trainData[\"chinese\"][-1] ) )\n",
    "print( len( trainData[\"english\"][-1] ) )\n",
    "model.fit( [trainData[\"chinese\"],\n",
    "            np.zeros( ( len( trainData[\"chinese\"] ),\n",
    "                        len( trainData[\"english\"][-1] ) ) ),\n",
    "            np.zeros( ( len( trainData[\"chinese\"] ),\n",
    "                        len( trainData[\"chinese\"][-1] ) ) )],\n",
    "            trainData[\"chinese\"] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
