{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Abstract\n",
    "\n",
    "The demo is a prototype of the project model. Codes and structure here could change in the future.\n",
    "The main point of codes below is to run on local computer and test whether it works on small scale of data.\n",
    "\n",
    "Now:\n",
    "    Prove that original model is inaccessible. Start to implemente original RNN-LSTM and treat it as baseline.\n",
    "\n",
    "TODO:\n",
    "    1. Try new way to approach the unsupervised method.\n",
    "    2. Try to understand how to control gradient in Tensorflow.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, Lambda, concatenate, Multiply\n",
    "from keras.models import Model\n",
    "import keras as K\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "K.backend.clear_session()\n",
    "KTF.set_session( tf.Session( config = tf.ConfigProto( device_count = {'gpu':0} ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( \"dogeA\" )\n",
    "# lmA = lm.loadLM( loadPath = \"./lm/chinese\", encoding = \"utf-8\" )\n",
    "# print( \"dogeB\" )\n",
    "# lmB = lm.loadLM( loadPath = \"./lm/english\", encoding = \"utf-8\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Get train data from path\n",
    "\n",
    "Read train data from files for each language and save to a dictionary.\n",
    "\n",
    "Args:\n",
    "    dataPath: file path of train data. Default value is \"../../Data/train/\".\n",
    "    langList: language list. Indicating which language data will be included in train data.\n",
    "              Default value is [\"Chinese\", \"English\"].\n",
    "    encoding: encoding of each file in train directory.\n",
    "    ratio: propotion of train data, others will be treat as dev data. Default value is 0.98.\n",
    "    sort: boolean value. If shuffle equals True, all data will be sorted according to their\n",
    "          length from short to long. Otherwise, train sentences will be shuffled at the end.\n",
    "          Default value is True.\n",
    "\n",
    "Returns:\n",
    "    trainData: a dictionary of train data sentences of each language. Its structure is:\n",
    "    \n",
    "               {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \n",
    "    devData: a dictionary of dev data sentences of each language. Its structure is:\n",
    "    \n",
    "               {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "\"\"\"\n",
    "def getTrainData( dataPath = \"../Data/train/\", lanList = [\"chinese\", \"english\"],\n",
    "                  encoding = \"UTF-8\", ratio = 0.98, sort = True ):\n",
    "    trainData = {}\n",
    "    devData   = {}\n",
    "    for lan in lanList:\n",
    "        print( lan )\n",
    "        if lan not in trainData:\n",
    "            trainData[lan] = []\n",
    "        if lan not in devData:\n",
    "            devData[lan] = []\n",
    "        files = os.listdir( dataPath + lan + \"/\" )\n",
    "        data = []\n",
    "        for file in files:\n",
    "            with open( dataPath + lan + \"/\" + file, encoding = encoding ) as f:\n",
    "                line = f.readline()\n",
    "                while line:\n",
    "                    wordList = line.split()\n",
    "                    data.append( [\"<S>\"] + wordList + [\"</S>\"] )\n",
    "                    line = f.readline()\n",
    "        # suffle here is to make sure that all data are random distributed\n",
    "        random.shuffle( data )\n",
    "        noOfSentences = len( data )\n",
    "        noOfTrainData = int( noOfSentences * ratio )\n",
    "        devData[lan]   = data[noOfTrainData:]\n",
    "        trainData[lan] = data[:noOfTrainData]\n",
    "        if sort == True:\n",
    "            trainData[lan].sort( key = lambda x: len( x ) )\n",
    "            devData[lan].sort( key = lambda x: len( x ) )\n",
    "    return trainData, devData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Generate dictionary and preprocess setences for each language\n",
    "\n",
    "Generate dictionary for each language and convert word to corresponding index.\n",
    "Here we set two dictionaries to speed up the whole program.\n",
    "\n",
    "Args:\n",
    "    data: a dictionary contains sentences of each language.  Its structure is:\n",
    "    \n",
    "          {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \n",
    "    threshold: a word will be replace with <UNK> if frequency of a word is\n",
    "               less than threshold. If the value is less than 1, it means\n",
    "               no need to replace any word to <UNK>. Default value is 0.\n",
    "\n",
    "Returns:\n",
    "    wordNumDict: a dictionary which can convert words to index in each language.\n",
    "                 Its structure is:\n",
    "                 \n",
    "                 {language A: {word A: index, word B: ..., ...}, language B: ..., ...}.\n",
    "    \n",
    "    numWordDict: a dictionary which can convert index to word in each language.\n",
    "                 Its structure is:\n",
    "                 \n",
    "                 {language A: {word A: index, word B: ..., ...}, language B: ..., ...}.\n",
    "\"\"\"\n",
    "def generateDict( data, threshold = 0 ):\n",
    "    wordNumDict = {}\n",
    "    numWordDict = {}\n",
    "    for lan, sentences in data.items():\n",
    "        wordCount = {}\n",
    "        if lan not in wordNumDict:\n",
    "            # Add special word to dictionary\n",
    "            wordNumDict[lan] = {\"<PAD>\": 0, \"<S>\": 1, \"</S>\": 2, \"<UNK>\": 3}\n",
    "        if lan not in numWordDict:\n",
    "            # Add special word to dictionary\n",
    "            numWordDict[lan] = {0: \"<PAD>\", 1: \"<S>\", 2: \"</S>\", 3: \"<UNK>\"}\n",
    "        \n",
    "        # Count word frequency\n",
    "        for sentence in sentences:\n",
    "            for i in range( len( sentence ) ):\n",
    "                word = sentence[i]\n",
    "                if word not in wordCount:\n",
    "                    wordCount[word] = 0\n",
    "                wordCount[word] += 1\n",
    "        \n",
    "        # Find and replace with <UNK>\n",
    "        for sentence in sentences:\n",
    "            for i in range( len( sentence ) ):\n",
    "                word = sentence[i]\n",
    "                if wordCount[word] < threshold:\n",
    "                    word = \"<UNK>\"\n",
    "                if word not in wordNumDict[lan]:\n",
    "                    number = len( wordNumDict[lan] )\n",
    "                    wordNumDict[lan][word] = number\n",
    "                    numWordDict[lan][number] = word\n",
    "                sentence[i] = wordNumDict[lan][word]\n",
    "    return wordNumDict, numWordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Number to One-hot\n",
    "\n",
    "Only convert sentences which length are small than 30.\n",
    "\n",
    "Args:\n",
    "    data: a dictionary contains sentences of each language.  Its structure is:\n",
    "    \n",
    "          {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \n",
    "    wordNumDict: a dictionary which can convert words to index in each language.\n",
    "                 Its structure is:\n",
    "                 \n",
    "                 {language A: {word A: index, word B: ..., ...}, language B: ..., ...}.\n",
    "\n",
    "Returns:\n",
    "    ndata: \n",
    "    td:\n",
    "\"\"\"\n",
    "def toCategory( data, wordNumDict, left, right ):\n",
    "#     n = right - left\n",
    "    maxlch = 0\n",
    "    maxlen = 0\n",
    "    n = 0\n",
    "    for i in range( left, right ):\n",
    "        if len( data[\"chinese\"][i] ) <= 30:\n",
    "            n += 1\n",
    "            maxlch = np.max( [maxlch, len( data[\"chinese\"][i] )] )\n",
    "            maxlen = np.max( [maxlen, len( data[\"english\"][i] )] )\n",
    "    if n == 0:\n",
    "        return False, [], []\n",
    "    zh = np.zeros( ( n, maxlch ) )\n",
    "    en = np.zeros( ( n, maxlen ) )\n",
    "    td = np.zeros( ( n, maxlen, len( wordNumDict[\"english\"] ) ) )\n",
    "    n = 0\n",
    "    for i in range( left, right ):\n",
    "        if len( data[\"chinese\"][i] ) <= 30:\n",
    "            for j in range( len( data[\"chinese\"][i] ) ):\n",
    "                zh[n, j] = data[\"english\"][i][j]\n",
    "            for j in range( len( data[\"english\"][i] ) ):\n",
    "                en[n, j] = data[\"english\"][i][j]\n",
    "                if j:\n",
    "                    w = data[\"english\"][i][j]\n",
    "                    td[n, j - 1, w] = 1\n",
    "            n += 1\n",
    "    ndata = {}\n",
    "    ndata[\"chinese\"] = zh\n",
    "    ndata[\"english\"] = en\n",
    "    return True, ndata, td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Simple Seqence to Sequence Implementation\n",
    "\n",
    "A simple implementation of Sequence to Sequence model. It works as baseline\n",
    "\n",
    "Args:\n",
    "    input_dim:  dimension of input word vector.\n",
    "    output_dim: dimension of output word vector.\n",
    "    hidden_dim: dimension of hidden states vector.\n",
    "    output_vocab_size: size of output language vocabulary size.\n",
    "    input_vocab_size:  size of input  language vocabulary size.\n",
    "    word_vec_dim: dimension of word-vector.\n",
    "    name: name of the model.\n",
    "\n",
    "Returns:\n",
    "    model: the whole model of simple Seq2Seq model.\n",
    "\n",
    "\"\"\"\n",
    "def simpleSeq2Seq( output_vocab_size, input_vocab_size, hidden_dim = 256,\n",
    "                   word_vec_dim = 512, name = \"demo\" ):\n",
    "    embedding_encoder  = Embedding( output_dim = word_vec_dim, input_dim = input_vocab_size,\n",
    "                                 name = name + \"_encoder_embedding\", mask_zero = True ) # \n",
    "    embedding_decoder = Embedding( output_dim = word_vec_dim, input_dim = output_vocab_size,\n",
    "                                 name = name + \"_decoder_embedding\", mask_zero = True ) # \n",
    "    # Encoder\n",
    "    encoder_input     = Input( shape = ( None, ), name = name + \"_encoder_input\" )\n",
    "    # change when using pre-trained embedding trainable= False\n",
    "    encoder           = LSTM( hidden_dim, return_state = True )\n",
    "    encoder_input_emb = embedding_encoder( encoder_input )\n",
    "    _, state_h, state_c = encoder( encoder_input_emb )\n",
    "    state_encoder     = [state_h, state_c]\n",
    "    # Decoder\n",
    "    decoder = LSTM( hidden_dim, return_sequences = True )\n",
    "\n",
    "    decoder_input     = Input( shape = ( None, ), name = name + \"_decoder_input\" )\n",
    "    decoder_input_emb = embedding_decoder( decoder_input )\n",
    "    decoder_outputs   = decoder( decoder_input_emb, initial_state = state_encoder )\n",
    "    decoder_dense     = Dense( output_vocab_size, activation = \"softmax\", name = name + \"_decoder_output\" )\n",
    "    decoder_outputs   = decoder_dense( decoder_outputs )\n",
    "\n",
    "    # Build model\n",
    "    model = Model( inputs = [encoder_input, decoder_input], outputs = decoder_outputs, name = name )\n",
    "    model.compile( optimizer = 'adam', loss = \"categorical_crossentropy\" )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, devData = getTrainData( \"../Data/train/\" )\n",
    "wordNumDict, numWordDict = generateDict( trainData, threshold = 5 )\n",
    "ivs = len( wordNumDict[\"chinese\"] )\n",
    "ovs = len( wordNumDict[\"english\"] )\n",
    "print( ivs, ovs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData[\"chinese\"] = trainData[\"chinese\"][::-1]\n",
    "devData[\"chinese\"] = devData[\"chinese\"][::-1]\n",
    "trainData[\"english\"] = trainData[\"english\"][::-1]\n",
    "devData[\"english\"] = devData[\"english\"][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simpleSeq2Seq( output_vocab_size = ovs, input_vocab_size = ivs, name = \"demo\" )\n",
    "model.summary()\n",
    "batch_size = 64\n",
    "\n",
    "losses = []\n",
    "n = 0\n",
    "for i in range( 0, len( trainData[\"chinese\"] ) + batch_size, batch_size ): \n",
    "    status, newTrainData, td = toCategory( trainData, wordNumDict, i, i + batch_size )\n",
    "    if status == False:\n",
    "        continue\n",
    "    loss = model.train_on_batch( [newTrainData[\"chinese\"], newTrainData[\"english\"]], td )\n",
    "    n += 1\n",
    "    print( n, loss )\n",
    "    losses.append( loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss( y_true, y_pred ):\n",
    "    y1 = y_pred[:, :256]\n",
    "    y2 = y_pred[:, 256:]\n",
    "    return y1 * y2\n",
    "#     y_rev = K.backend.reverse( y_pred, axes = -1 )\n",
    "#     loss = K.backend.sum( y_pred * y_rev / 2, axis = -1, keepdims = True )\n",
    "#     return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dual Neural Machine Translation Model\n",
    "\n",
    "The structure of the model is:\n",
    "\n",
    "Input A ---> RNN1 ---> Output B ---> RNN2 ---> Output A\n",
    "                          |                       |\n",
    "                          V                       V\n",
    "                        LM B                    LM A\n",
    "\n",
    "Args:\n",
    "    langA_dim:  dimension of language A word vector.\n",
    "    langB_dim:  dimension of language B word vector.\n",
    "    hidden_dim: dimension of hidden state vector.\n",
    "    lmA: language model of language A.\n",
    "    lmB: language model of language B.\n",
    "\n",
    "Returns:\n",
    "    model: the whole model of Dual Neural Machine Translation Model.\n",
    "\"\"\"\n",
    "def dualNMTModel( langA_vocab_size = 100000, langB_vocab_size = 100000,\n",
    "                  langA_dim = 1024, langB_dim = 1024, word_vec_dim = 256, hidden_dim = 256,\n",
    "                  name = \"demo\" ):\n",
    "    # first part of translation model from language A to language B\n",
    "    embedding_A2B = Embedding( output_dim = word_vec_dim, input_dim = langA_vocab_size,\n",
    "                               name = name + \"_embedding_A2B\", mask_zero = True )\n",
    "    to_label = Lambda( lambda x: K.backend.argmax( x, axis = 2 ) )\n",
    "    avg_emb  = Lambda( lambda x: tf.math.reduce_mean( x, axis = 1 ) )\n",
    "    rev_emb  = Lambda( lambda x: K.backend.reverse( x, axes = 1 ) )\n",
    "    cos_sim  = Lambda( lambda x, y: x * y )\n",
    "    # Encoder\n",
    "    encoder_input_A2B     = Input( shape = ( None, ), name = name + \"_encoder_input_A2B\" )\n",
    "    # change when using pre-trained embedding trainable= False\n",
    "    encoder_A2B           = LSTM( hidden_dim, return_state = True )\n",
    "    encoder_input_emb_A2B = embedding_A2B( encoder_input_A2B )\n",
    "    _, state_h, state_c   = encoder_A2B( encoder_input_emb_A2B )\n",
    "    state_encoder_A2B     = [state_h, state_c]\n",
    "    # Decoder\n",
    "    decoder_A2B = LSTM( hidden_dim, return_sequences = True )\n",
    "\n",
    "    decoder_input_A2B     = Input( shape = ( None, ), name = name + \"_decoder_input_A2B\" )\n",
    "    decoder_input_emb_A2B = embedding_A2B( decoder_input_A2B )\n",
    "    decoder_outputs_A2B   = decoder_A2B( decoder_input_emb_A2B, initial_state = state_encoder_A2B )\n",
    "    decoder_dense_A2B     = Dense( langB_vocab_size, activation = \"softmax\", name = name + \"_decoder_output_A2B\" )\n",
    "    decoder_outputs_A2B   = decoder_dense_A2B( decoder_outputs_A2B )\n",
    "    \n",
    "    # language model <- langB_output\n",
    "    # lossB = Lambda( perplexity )( langB_output )\n",
    "    \n",
    "    # second part of translation model from another language to original language\n",
    "    # first part of translation model from language A to language B\n",
    "    embedding_B2A = Embedding( output_dim = word_vec_dim, input_dim = langB_vocab_size,\n",
    "                               name = name + \"_embedding_B2A\", mask_zero = True )\n",
    "    # Encoder\n",
    "    encoder_input_B2A     = to_label( decoder_outputs_A2B )\n",
    "    # change when using pre-trained embedding trainable= False\n",
    "    encoder_B2A           = LSTM( hidden_dim, return_state = True )\n",
    "    encoder_input_emb_B2A = embedding_B2A( encoder_input_B2A )\n",
    "#     _, state_h, state_c   = encoder_B2A( decoder_outputs_A2B )\n",
    "    _, state_h, state_c   = encoder_B2A( encoder_input_emb_B2A )\n",
    "    state_encoder_B2A     = [state_h, state_c]\n",
    "    # Decoder\n",
    "    decoder_B2A = LSTM( hidden_dim, return_sequences = True )\n",
    "\n",
    "    decoder_input_B2A     = Input( shape = ( None, ), name = name + \"_decoder_input_B2A\" )\n",
    "    decoder_input_emb_B2A = embedding_B2A( decoder_input_B2A )\n",
    "    decoder_outputs_B2A   = decoder_B2A( decoder_input_emb_B2A, initial_state = state_encoder_B2A )\n",
    "    decoder_dense_B2A     = Dense( langA_vocab_size, activation = \"softmax\", name = name + \"_decoder_output_B2A\" )\n",
    "    decoder_outputs_B2A   = decoder_dense_B2A( decoder_outputs_B2A )\n",
    "    \n",
    "    avg_input_emb  = avg_emb( encoder_input_emb_A2B )\n",
    "    decoder_outputs_label = to_label( decoder_outputs_B2A )\n",
    "    decoder_outputs_emb   = embedding_A2B( decoder_outputs_label )\n",
    "    avg_output_emb = avg_emb( decoder_outputs_emb )\n",
    "#     avg_output_emb = rev_emb( avg_output_emb )\n",
    "    \n",
    "    output = Multiply()( [avg_input_emb, avg_output_emb] )\n",
    "    \n",
    "#     output = concatenate( [avg_input_emb, avg_output_emb], axis = 1 )\n",
    "    \n",
    "    # language model <- langA_output\n",
    "    # lossA = Lambda( perplexity )( langA_output )\n",
    "    \n",
    "    # Build model\n",
    "    model = Model( inputs = [encoder_input_A2B, decoder_input_A2B, decoder_input_B2A],\n",
    "                   outputs = output ) #[decoder_outputs_B2A, decoder_outputs_A2B]\n",
    "    model.compile( optimizer = 'adam', loss = lambda y_true, y_pred: y_pred ) #, loss_weights = [0.5, 1.]\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"dogeLM\" )\n",
    "model = dualNMTModel( langA_vocab_size = 37100, langB_vocab_size = 27869 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, devData = getTrainData( \"../Data/test/\" )\n",
    "wordNumDict, numWordDict = generateDict( trainData )\n",
    "print( len( trainData[\"chinese\"][-1] ) )\n",
    "print( len( trainData[\"english\"][-1] ) )\n",
    "model.fit( [trainData[\"chinese\"],\n",
    "            np.zeros( ( len( trainData[\"chinese\"] ),\n",
    "                        len( trainData[\"english\"][-1] ) ) ),\n",
    "            np.zeros( ( len( trainData[\"chinese\"] ),\n",
    "                        len( trainData[\"chinese\"][-1] ) ) )],\n",
    "            trainData[\"chinese\"] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
