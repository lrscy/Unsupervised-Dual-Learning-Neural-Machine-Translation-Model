{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-Level Language Modeling\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get all files name under path\n",
    "\n",
    "Args:\n",
    "    path: folder path to retrieve files' name.\n",
    "    ratio: propotion of training data. Default value is 1 (100%).\n",
    "    shuffle: a boolean value. TRUE: shuffle list; False: order list.\n",
    "\n",
    "Returns:\n",
    "    filesName[:train]: a list of all files end with \".txt\" for training set. For example:\n",
    "\n",
    "    [\"dir/a.txt\", \"dir/b.txt\"].\n",
    "\n",
    "    filesName[train:]: a list of all files end with \".txt\" for held-out set. For example:\n",
    "\n",
    "    [\"dir/a.txt\", \"dir/b.txt\"].\n",
    "\"\"\"\n",
    "def getFilesName( path, ratio = 1, shuffle = False ):\n",
    "    print( \"Retrieving files name from folder %s...\" % ( path ) )\n",
    "    filesName = []\n",
    "    files = os.listdir( path )\n",
    "    for file in files:\n",
    "        name = '/'.join( [path, file] )\n",
    "        filesName.append( name )\n",
    "    if shuffle:\n",
    "        random.shuffle( filesName )\n",
    "    else:\n",
    "        filesName.sort()\n",
    "    total = len( filesName )\n",
    "    train = int( total * ratio )\n",
    "    return filesName[:train], filesName[train:]\n",
    "\n",
    "\"\"\"Preprocess data\n",
    "\n",
    "Find words in training set that appear ≤ 5 times as “UNK”.\n",
    "\n",
    "Note: The function will figure out all words which are need to be replaced by \"UNK\"\n",
    "      and they will be replaced when building n-gram word-level language model.\n",
    "\n",
    "Args:\n",
    "    contents: a list of content to be processed. Content is also a list.\n",
    "\n",
    "Returns:\n",
    "    repc: a list of words that need to be replaced with \"<UNK>\". For example:\n",
    "    \n",
    "    \"[word a, word b, word c]\"\n",
    "\"\"\"\n",
    "def unk( contents ):\n",
    "    d = {}\n",
    "    for content in contents:\n",
    "        for w in content:\n",
    "            if w not in d:\n",
    "                d[w] = 0\n",
    "            d[w] += 1\n",
    "    repw = {}\n",
    "    for ( k, v ) in d.items():\n",
    "        if v <= 3:\n",
    "            if k not in repw:\n",
    "                repw[k] = 0\n",
    "            repw[k] += 1\n",
    "    return repw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Character-Level Language Model\n",
    "\"\"\"Generate n-gram dictionary.\n",
    "\n",
    "Generate n-gram dictionary based on fed string and n.\n",
    "\n",
    "Note: replace characters in content with \"?\" which represents \"UNK\" in character-level.\n",
    "\n",
    "Args:\n",
    "    contents: a list of content used to calculate ngrams.\n",
    "    n: n-gram.\n",
    "    d: language model corresponds to n-gram.\n",
    "\n",
    "Returns:\n",
    "    None.\n",
    "\"\"\"\n",
    "def ngrams( contents, n, d ):\n",
    "    for content in contents:\n",
    "        for i in range( n - 1, len( content ) - 1 ):\n",
    "            k = ' '.join( content[i - n + 1:i + 1] )\n",
    "            if k not in d[\"c\"]:\n",
    "                d[\"c\"][k] = 0\n",
    "            d[\"c\"][k] += 1\n",
    "            d[\"t\"] += 1\n",
    "\n",
    "\"\"\"Build language model\n",
    "\n",
    "Preprocess files and across all files in the directory (counted together), report the \n",
    "unigram, bigram, and trigram character counts.\n",
    "\n",
    "Args:\n",
    "    content: a list contains content needed to be processed.\n",
    "\n",
    "Returns:\n",
    "    lm: a dictionary of language model when savePath equals empty string. Its structure is:\n",
    "    \n",
    "    {\"unigram\": {\"c\": unigram, \"t\": total unigram characters},\n",
    "     \"bigram\" : {\"c\": bigram,  \"t\": total bigram  characters},\n",
    "     \"trigram\": {\"c\": trigram, \"t\": total trigram characters}}.\n",
    "\"\"\"\n",
    "def LM( contents ):\n",
    "    print( \"Building language modeling...\" )\n",
    "    lm = {\"unigram\": {\"c\": {}, \"t\": 0},\n",
    "          \"bigram\" : {\"c\": {}, \"t\": 0},\n",
    "          \"trigram\": {\"c\": {}, \"t\": 0}}\n",
    "    ngram = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "    \n",
    "    # Calculate unigram, bigram, and trigram\n",
    "    print( \"Calculating n-grams...\" )\n",
    "    ngrams( contents, 1, lm[\"unigram\"] )\n",
    "    ngrams( contents, 2, lm[\"bigram\" ] )\n",
    "    ngrams( contents, 3, lm[\"trigram\"] )\n",
    "    return lm\n",
    "\n",
    "\"\"\"Build Language Model\n",
    "\n",
    "Across all files in the directory (counted together), report the unigram, bigram, and trigram\n",
    "character counts and save them in seperate files.\n",
    "\n",
    "Args:\n",
    "    trainDataPath: train data path.\n",
    "    encoding: train data files' encoding\n",
    "    savePath: path to save language model.\n",
    "    ratio: the proportion of the real training set comparing to whole training set\n",
    "\n",
    "Returns:\n",
    "    None\n",
    "\"\"\"\n",
    "def buildLM( trainDataPath = \"./train\", encoding = \"Latin-1\", savePath = \"./lm\", ratio = 1 ):\n",
    "    ngram = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "    trainFiles, heldOutFiles = getFilesName( trainDataPath )\n",
    "    # preprocess data and find UNK\n",
    "    print( \"Counting for finding UNK.\")\n",
    "    contents = []\n",
    "    for fileName in trainFiles:\n",
    "        with open( fileName, 'r', encoding = encoding ) as f:\n",
    "            content = []\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                contents.append( line.split() )\n",
    "                line = f.readline()\n",
    "    repw = unk( contents )\n",
    "    if len( repw ):\n",
    "        for content in contents:\n",
    "            for i in range( len( content ) ):\n",
    "                if content[i] in repw:\n",
    "                    content[i] = \"<UNK>\"\n",
    "    lm = LM( contents )\n",
    "    if not os.path.isdir( savePath ):\n",
    "        os.makedirs( savePath )\n",
    "    for name in ngram:\n",
    "        print( name )\n",
    "        with open( savePath + \"/\" + name, \"w\", encoding = encoding ) as f:\n",
    "            f.write( str( lm[name][\"t\"] ) + \"\\n\" )\n",
    "            for ( k, v ) in lm[name][\"c\"].items():\n",
    "                f.write( k + \" \" + str( v ) + \"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving files name from folder ../../Data/train/english...\n",
      "Counting for finding UNK.\n",
      "Building language modeling...\n",
      "Calculating n-grams...\n",
      "unigram\n",
      "bigram\n",
      "trigram\n"
     ]
    }
   ],
   "source": [
    "buildLM( trainDataPath = \"../../Data/train/english\", encoding = \"UTF-8\", savePath = \"./lm/english\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Linear Interplotation smoothing function on language model.\n",
    "\"\"\"Linear Interplotation Smoothing\n",
    "\n",
    "P(w_{n}|w_{n-2}w_{n-1}) = lambda3 * P(w_{n}|w_{n-2}w_{n-1}) +\n",
    "                          lambda2 * P(w_{n}|w_{n-1}) +\n",
    "                          lambda1 * P(w_{n})\n",
    "    where lambda1 + lambda2 + lambda3 = 1.\n",
    "\n",
    "Args:\n",
    "    lm: a dictionary contains language model. Its structure is:\n",
    "    \n",
    "    {\"unigram\": {\"c\": unigram, \"t\": total unigram characters},\n",
    "     \"bigram\" : {\"c\": bigram,  \"t\": total bigram  characters},\n",
    "     \"trigram\": {\"c\": trigram, \"t\": total trigram characters}}.\n",
    "\n",
    "    lambdas: a dictionary of lambda for interplotation or addLambda. Its structure is:\n",
    "    \n",
    "    {1: lambdaForUnigram, 2: lambdaForBigram, 3: lambdaForTrigram}.\n",
    "\n",
    "    s: string wating for calculating unigram, bigram, and trigram.\n",
    "\n",
    "Returns:\n",
    "    p: a double number represents the final probability of P(w_{n}|w_{n-2}w_{n-1}).\n",
    "\"\"\"\n",
    "def interplotation( lm, lambdas, s ):\n",
    "    s1 = s[2:]\n",
    "    s2 = s[1:]\n",
    "    s3 = s[0:]\n",
    "    if s1 not in lm[\"unigram\"][\"c\"]:\n",
    "        p1 = lm[\"unigram\"][\"c\"][\"?\"] / lm[\"unigram\"][\"t\"]\n",
    "    else:\n",
    "        p1 = lm[\"unigram\"][\"c\"][s1] / lm[\"unigram\"][\"t\"]\n",
    "    if s2 not in lm[\"bigram\"][\"c\"] or s1 not in lm[\"unigram\"][\"c\"]:\n",
    "        p2 = 0\n",
    "    else:\n",
    "        p2 = lm[\"bigram\" ][\"c\"][s2] / lm[\"unigram\" ][\"c\"][s1]\n",
    "    if s3 not in lm[\"trigram\"][\"c\"] or s2 not in lm[\"bigram\"][\"c\"]:\n",
    "        p3 = 0\n",
    "    else:\n",
    "        p3 = lm[\"trigram\"][\"c\"][s3] / lm[\"bigram\"][\"c\"][s2]\n",
    "    p = lambdas[1] * p1 + lambdas[2] * p2 + lambdas[3] * p3\n",
    "    return p\n",
    "\n",
    "\"\"\"Calculate perplexity\n",
    "\n",
    "PP(W) = P(w_1w_2 ... w_n)^(-1/n)\n",
    "      = 2^{-1 / n * sum_{i=1:n}(log2(LM(w_i|w_{i-2}w_{i-1})))}\n",
    "\n",
    "Note: Since here is no <SOS> and <EOS> in language model, n would be the length of\n",
    "      the content - 2.\n",
    "\n",
    "Args:\n",
    "    content: string content.\n",
    "    lm: a dictionary contains language model. Its structure is:\n",
    "    \n",
    "    {\"unigram\": {\"c\": unigram, \"t\": total unigram characters},\n",
    "     \"bigram\" : {\"c\": bigram,  \"t\": total bigram  characters},\n",
    "     \"trigram\": {\"c\": trigram, \"t\": total trigram characters}}.\n",
    "\n",
    "    **kwargs:\n",
    "        func: smoothing function name on calculating P(w_i|w_{i-2}w_{i-1}), including\n",
    "              func = \"Interplotation\" and func = \"AddLambda\".\n",
    "        \n",
    "        lambdas: a dictionary of lambda for interplotation or addLambda. Its structure is:\n",
    "    \n",
    "        {1: lambdaForUnigram, 2:lambdaForBigram, 3:lambdaForTrigram}\n",
    "        \n",
    "        When using addLambda function, only need to feed one specific lambda.\n",
    "    \n",
    "Returns:\n",
    "    ppw: a double number represents the perplexity of the content.\n",
    "\n",
    "Raise:\n",
    "    KeyError: an error when trying to find smoothing function.\n",
    "\"\"\"\n",
    "def perplexity( content, lm, **kwargs ):\n",
    "    length = len( content )\n",
    "    log2p = 0\n",
    "    if( length <= 2 ):\n",
    "        raise Exception( \"Too short content.\" )\n",
    "    if \"func\" in kwargs:\n",
    "        if kwargs[\"func\"] == \"Interplotation\":\n",
    "            for i in range( length - 1 ):\n",
    "                p = interplotation( lm, kwargs[\"lambdas\"], content[i:i + 3] )\n",
    "                log2p += math.log2( p )\n",
    "        elif kwargs[\"func\"] == \"AddLambda\":\n",
    "            for i in range( length - 1 ):\n",
    "                p = addLambda( lm, kwargs[\"lambdas\"], content[i:i + 3] )\n",
    "                log2p += math.log2( p )\n",
    "        else:\n",
    "            raise Exception( \"Cannot find the smoothing function.\" )\n",
    "    else:\n",
    "        raise Exception( \"No smoothing function.\" )\n",
    "    log2p *= -1 / ( length - 2 )\n",
    "    ppw = 2 ** log2p\n",
    "    return ppw\n",
    "\n",
    "\"\"\"Grid search\n",
    "\n",
    "Using grid search and held-out data set find the best lambdas for\n",
    "linear interplotation smoothing.\n",
    "\n",
    "Args:\n",
    "    lambdas: a generator of lambdas which generate a dictionary of lambdas\n",
    "             for unigram, bigram, trigram each time. For example:\n",
    "    \n",
    "    {1:0.1, 2:0.1, 3:0.8}\n",
    "    \n",
    "    lm: a dictionary of language model. Its structure is:\n",
    "    \n",
    "    {\"unigram\": {\"c\": unigram, \"t\": total unigram characters},\n",
    "     \"bigram\" : {\"c\": bigram,  \"t\": total bigram  characters},\n",
    "     \"trigram\": {\"c\": trigram, \"t\": total trigram characters}}.\n",
    "    \n",
    "    heldOutFiles: files name belongs to held-out data set.\n",
    "\n",
    "Returns:\n",
    "    lambdas: the best lambdas combination.\n",
    "\"\"\"\n",
    "def gridSearch( lambdas, lm, heldOutFiles ):\n",
    "    print( \"Applying grid search...\" )\n",
    "    minAvg = float( \"inf\" )\n",
    "    for lambd in lambdas:\n",
    "        avg = 0\n",
    "        for name in heldOutFiles:\n",
    "            content = preprocess( name )\n",
    "            avg += perplexity( content, lm, func = \"Interplotation\", lambdas = lambd )\n",
    "        if( avg < minAvg ):\n",
    "            minAvg = avg\n",
    "            bestLambda = lambd\n",
    "    return bestLambda\n",
    "\n",
    "\"\"\"Calculate the perplexity with interplotation smoothing method\n",
    "\n",
    "Calculate the perplexity for each file in the test set using linear interpolation smoothing method.\n",
    "\n",
    "Note: Here I sperate the training data and held-out data by seperating number of files rather\n",
    "      than seperating all content after concatenating all together and then dividing them.\n",
    "      \n",
    "      There are two reasons to divide data in this way\n",
    "      1. It is hard and cumbersome to measure 80% of the content just on name list.\n",
    "      2. If loading all content into memory at the same time, it is too time consuming and\n",
    "         wastes time without obviously improvment on final language model.\n",
    "\n",
    "Args:\n",
    "    trainDataPath: train data path.\n",
    "    encoding: train data files' encoding\n",
    "    savePath: path to save language model. If it equals to empty string, the function returns\n",
    "              language model.\n",
    "    testDataPath: test data path.\n",
    "    ratio: the proportion of the real training set comparing to whole training set.\n",
    "\n",
    "Returns:\n",
    "    None.\n",
    "\"\"\"\n",
    "def interplotationPPW( trainDataPath = \"./train\", encoding = \"Latin-1\", savePath = \"./save\",\n",
    "                       testDataPath = \"./test\", ratio = 0.8 ):\n",
    "    # Get new language model\n",
    "    trainFiles, heldOutFiles = getFilesName( trainDataPath, ratio = ratio, shuffle = True )\n",
    "    content = \"\"\n",
    "    contents = []\n",
    "    for fileName in trainFiles:\n",
    "        content += preprocess( fileName, encoding )\n",
    "        contents.append( content )\n",
    "    repc = unk( content )\n",
    "    if len( repc ) > 2:\n",
    "        for i in range( len( contents ) ):\n",
    "            contents[i] = re.sub( repc, \"?\", contents[i] )\n",
    "    lm = LM( contents )\n",
    "\n",
    "    # Choose lambdas by grid search and perplexity\n",
    "    lambdas = ( {1: x / 10, 2: y / 10, 3: ( 10 - x - y ) / 10}\n",
    "                   for x in range( 1, 10, 1 ) for y in range( 1, 10 - x, 1 ) )\n",
    "    lambdas = gridSearch( lambdas, lm, heldOutFiles )\n",
    "\n",
    "    # File-PPW pair dictionary\n",
    "    dfp = {}\n",
    "    filesName, _ = getFilesName( testDataPath )\n",
    "    for fileName in filesName:\n",
    "        content = preprocess( fileName )\n",
    "        ppw = perplexity( content, lm, func = \"Interplotation\", lambdas = lambdas )\n",
    "        dfp[fileName] = ppw\n",
    "    fps = sorted( dfp.items(), key = lambda x: x[1], reverse = True )\n",
    "    with open( savePath + \"/\" + \"filesPerplexity-interplotation.txt\", 'w' ) as f:\n",
    "        for fp in fps:\n",
    "            f.write( fp[0].split( '/' )[-1] + \", \" + str( fp[1] ) + \"\\n\" )\n",
    "\n",
    "# Apply Add-Lambda smoothing function on language model.\n",
    "\"\"\"Load language model\n",
    "\n",
    "Load language model from folder \"lm\" and save them into dictionary \"lm\".\n",
    "\n",
    "Args:\n",
    "    loadPath: language model load path.\n",
    "    encoding: language model files' encoding\n",
    "\n",
    "Returns:\n",
    "    lm: a dictionary of language model. Its structure is:\n",
    "    \n",
    "    {\"unigram\": {\"c\": unigram, \"t\": total unigram characters},\n",
    "     \"bigram\" : {\"c\": bigram,  \"t\": total bigram  characters},\n",
    "     \"trigram\": {\"c\": trigram, \"t\": total trigram characters}}.\n",
    "\"\"\"\n",
    "def loadLM( loadPath = \"./lm\", encoding = \"utf-8\" ):\n",
    "    lm = {}\n",
    "    ngram = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "    n = 0\n",
    "    # load unigram, bigram, and trigram\n",
    "    for name in ngram:\n",
    "        n += 1\n",
    "        with open( loadPath + \"/\" + name, \"r\", encoding = encoding ) as f:\n",
    "            ngram = {}\n",
    "            total = 0\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                kv = line.split( ' ' )\n",
    "                if len( kv ) > 1:\n",
    "                    kv[0] = line[:n]\n",
    "                    kv[1] = line[n + 1:]\n",
    "                    ngram[kv[0]] = int( kv[1] )\n",
    "                else:\n",
    "                    total = int( kv[0] )\n",
    "                line = f.readline()\n",
    "            lm[name] = {\"c\": ngram, \"t\": total}\n",
    "    return lm\n",
    "\n",
    "\"\"\"Add lambda smoothing\n",
    "\n",
    "P(w_{n}|w_{n-1}w_{n-2}) = ( c(w_{n-1}w_{n-2}, w_{n}) + lambda ) /\n",
    "                            ( c(w_{n-1}w_{n-1}) + lambda * V )\n",
    "\n",
    "Args:\n",
    "    lm: a dictionary of language model. Its structure is:\n",
    "    \n",
    "    {\"unigram\": {\"c\": unigram, \"t\": total unigram characters},\n",
    "     \"bigram\" : {\"c\": bigram,  \"t\": total bigram  characters},\n",
    "     \"trigram\": {\"c\": trigram, \"t\": total trigram characters}}.\n",
    "    \n",
    "    lambdas: a generator of lambdas which generate a dictionary of lambdas\n",
    "             for unigram, bigram, trigram each time. For example:\n",
    "    \n",
    "    {1:0.1, 2:0.1, 3:0.8}\n",
    "    \n",
    "    s: string wating for calculating unigram, bigram, and trigram.\n",
    "\n",
    "Returns:\n",
    "    p: a double number represents the final probability of P(w_{n}|w_{n-2}w_{n-1}).\n",
    "\"\"\"\n",
    "def addLambda( lm, lambdas, s ):\n",
    "    if s not in lm[\"trigram\"][\"c\"]:\n",
    "        cnt1 = 0\n",
    "    else:\n",
    "        cnt1 = lm[\"trigram\"][\"c\"][s]\n",
    "    if s[:2] not in lm[\"bigram\"][\"c\"]:\n",
    "        cnt2 = 0\n",
    "    else:\n",
    "        cnt2 = lm[\"bigram\"][\"c\"][s[:2]]\n",
    "    p = ( cnt1 + lambdas[3] ) / ( cnt2 + len( lm[\"bigram\"][\"c\"] ) * lambdas[3] )\n",
    "    return p\n",
    "\n",
    "\"\"\"Main function for problem 3.3\n",
    "\n",
    "Calculate the perplexity for each \n",
    "le in the test set using linear interpolation smoothing\n",
    "method.\n",
    "\n",
    "Args:\n",
    "    trainDataPath: train data path.\n",
    "    encoding: train data files' encoding\n",
    "    savePath: path to save language model. If it equals to empty string, the function returns\n",
    "              language model.\n",
    "    testDataPath: test data path.\n",
    "    ratio: the proportion of the real training set comparing to whole training set.\n",
    "\n",
    "Returns:\n",
    "    None.\n",
    "\"\"\"\n",
    "def addLambdaPPW( lmPath = \"./lm\", encoding = \"Latin-1\", savePath = \"./3_3\",\n",
    "                  testDataPath = \"./test\" ):\n",
    "    # Get new language model\n",
    "    lm = loadLM( lmPath, encoding )\n",
    "    lambdas = {3: 0.1}\n",
    "    # File-PPW pair dictionary\n",
    "    dfp = {}\n",
    "    filesName, _ = getFilesName( testDataPath )\n",
    "    for fileName in filesName:\n",
    "        content = preprocess( fileName )\n",
    "        ppw = perplexity( content, lm, func = \"AddLambda\", lambdas = lambdas )\n",
    "        dfp[fileName] = ppw\n",
    "    fps = sorted( dfp.items(), key = lambda x: x[1], reverse = True )\n",
    "    with open( savePath + \"/\" + \"filesPerplexity-addLambda.txt\", 'w' ) as f:\n",
    "        for fp in fps:\n",
    "            f.write( fp[0].split( '/' )[-1] + \", \" + str( fp[1] ) + \"\\n\" )\n",
    "\n",
    "# Test\n",
    "def test():\n",
    "    # Build Character-Level Language Model\n",
    "    print( \"Building Character-Level Language Model...\" )\n",
    "    buildLM( trainDataPath = \"./train\", encoding = \"Latin-1\", savePath = \"./lm\",\n",
    "             ratio = 1 )\n",
    "    # Apply Linear Interplotation smoothing function on language model\n",
    "    print( \"Applying Linear Interplotation smoothing function on language model...\" )\n",
    "    interplotationPPW( trainDataPath = \"./train\", encoding = \"Latin-1\",\n",
    "                       savePath = \"./save\", testDataPath = \"./test\", ratio = 0.8 )\n",
    "    # Apply Add-Lambda smoothing function on language model\n",
    "    print( \"Applying Add-Lambda smoothing function on language model...\" )\n",
    "    addLambdaPPW( lmPath = \"./lm\", encoding = \"Latin-1\", savePath = \"./save\",\n",
    "                  testDataPath = \"./test\" )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    groupsmoothing = parser.add_mutually_exclusive_group()\n",
    "    groupsmoothing.add_argument( \"-t\", \"--test\", help = \"Test on all funcitions\", action=\"store_true\" )\n",
    "    groupsmoothing.add_argument( \"-b\", \"--build\", help = \"Build a character-level n-gram language model\", \n",
    "                                 action=\"store_true\" )\n",
    "    groupsmoothing.add_argument( \"-i\", \"--interplotation\",\n",
    "                                 help = \"Apply Linear Interplotation smoothing function on language model\",\n",
    "                                 action=\"store_true\" )\n",
    "    groupsmoothing.add_argument( \"-a\", \"--addLambda\",\n",
    "                                 help = \"Apply Add-Lambda smoothing function on language model\",\n",
    "                                 action=\"store_true\" )\n",
    "    parser.add_argument( \"-e\", \"--encoding\", type = str,\n",
    "                         help = \"Encoding of files\", default = \"Latin-1\" )\n",
    "    parser.add_argument( \"-r\", \"--ratio\", type = float,\n",
    "                         help = \"proportion of real train data files in train data path\",\n",
    "                         default = 1.0 )\n",
    "    parser.add_argument( \"--trainPath\", type = str, help = \"Path that train data stores\",\n",
    "                         default = \"./train\" )\n",
    "    parser.add_argument( \"--testPath\",  type = str, help = \"Path that test data stores\",\n",
    "                         default = \"./test\" )\n",
    "    parser.add_argument( \"--savePath\",  type = str, help = \"Path that function result will save at\",\n",
    "                         default = \"./save\" )\n",
    "    parser.add_argument( \"--lmPath\",    type = str, help = \"Path that language model stores\",\n",
    "                         default = \"./lm\" )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.test:\n",
    "        test()\n",
    "    if args.build:\n",
    "        buildLM( args.trainPath, args.encoding, args.savePath, args.ratio )\n",
    "    if args.interplotation:\n",
    "        interplotationPPW( args.trainPath, args.encoding, args.savePath, args.testPath, args.ratio )\n",
    "    if args.addLambda:\n",
    "        addLambdaPPW( args.lmPath, args.encoding, args.savePath, args.testPath )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
