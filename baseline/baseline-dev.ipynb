{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import multiprocessing\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.backend.clear_session()\n",
    "# sess = tf.Session( config = tf.ConfigProto( device_count = {'gpu':0} ) )\n",
    "# KTF.set_session( sess )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global debug parameter\n",
    "\n",
    "# open embedding layer or not\n",
    "EMB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors( path, word2vec, language ):\n",
    "    files = os.listdir( path + language + \"/\" )\n",
    "    data = {}\n",
    "    for file in files:\n",
    "        fin = io.open( path + file, \"r\", encoding = \"utf-8\",\n",
    "                       newline = \"\\n\", errors = \"ignore\" )\n",
    "        n, d = map( int, fin.readline().split() )\n",
    "        for line in fin:\n",
    "            tokens = line.rstrip().split( ' ' )\n",
    "            data[tokens[0]] = map( float, tokens[1:] )\n",
    "    word2vec[language] = data\n",
    "\n",
    "def load_word2vec( path, language_list = [\"chinese\", \"english\"] ):\n",
    "    p = []\n",
    "    manager = multiprocessing.Manager()\n",
    "    word2vec = manager.dict()\n",
    "    for language in language_list:\n",
    "        # Only one file in each folder\n",
    "        p_lan = multiprocessing.Process( target = load_vectors,\n",
    "                                         args = ( path, word2vec, language ) )\n",
    "        p.append( p_lan )\n",
    "        p_lan.start()\n",
    "    for p_lan in p:\n",
    "        p_lan.join()\n",
    "    word2vec = dict( word2vec )\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_data( path, language, encoding, lan_data = {} ):\n",
    "    sentences = []\n",
    "    files = os.listdir( path + language + \"/\" )\n",
    "    for file in files:\n",
    "        with open( path + language + \"/\" + file, \"r\", encoding = encoding ) as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                line = line.strip()\n",
    "                if len( line ) == 0:\n",
    "                    line = f.readline()\n",
    "                    continue\n",
    "                words = [\"<S>\"] + line.split() + [\"</S>\"]\n",
    "                sentences.append( words )\n",
    "                line = f.readline()\n",
    "    lan_data[language] = sentences\n",
    "\n",
    "def shuffle_list( x, shuf_order ):\n",
    "    x = np.array( x )[shuf_order].tolist()\n",
    "    return x\n",
    "\n",
    "def get_data( path = \"../data/\", language_list = [\"chinese\", \"english\"],\n",
    "              encoding_list = [\"UTF-8\", \"UTF-8\"], shuffle = True ):\n",
    "    \"\"\"Get data from path\n",
    "\n",
    "    Args:\n",
    "        path: a string represents corpus path of each language.\n",
    "        language_list: a list of string represents languages.\n",
    "        encoding_list: a list of string represents encoding of each language\n",
    "                       corresponding to language list.\n",
    "        shuffle: a boolean value. True for shuffle.\n",
    "\n",
    "    Returns:\n",
    "        lan_data: a dictionary contains sentences of each language.\n",
    "                  Its structure is:\n",
    "\n",
    "                  {language A: [[word1, word2, ...], [...], ...],\n",
    "                   language B: ...}\n",
    "    \"\"\"\n",
    "    assert len( language_list ) == len( encoding_list )\n",
    "    p = []\n",
    "    manager = multiprocessing.Manager()\n",
    "    lan_data = manager.dict()\n",
    "    # Read parallel corpus\n",
    "    for lan, enc in zip( language_list, encoding_list ):\n",
    "        # I don't know why I should pre-define it, but it works.\n",
    "        # If I remove this line, some lines in data will be overwritten by unkown data.\n",
    "        lan_data[lan] = {}\n",
    "        print( \"Reading \" + lan + \" language corpus...\" )\n",
    "        p_lan = multiprocessing.Process( target = get_language_data,\n",
    "                                         args = ( path, lan, enc, lan_data ) )\n",
    "        p.append( p_lan )\n",
    "        p_lan.start()\n",
    "    for p_lan in p:\n",
    "        p_lan.join()\n",
    "    lan_data = dict( lan_data )\n",
    "    \n",
    "    if shuffle == True:\n",
    "        print( \"Shuffling...\" )\n",
    "        \n",
    "        # Decide shuffle order\n",
    "        length = len( lan_data[language_list[0]] )\n",
    "        shuf_order = [i for i in range( length )]\n",
    "        random.shuffle( shuf_order )\n",
    "\n",
    "        # Shuffle corpus\n",
    "        pool = multiprocessing.Pool( processes = len( language_list ) )\n",
    "        p = {}\n",
    "        for language in language_list:\n",
    "            p_lan = pool.apply_async( func = shuffle_list,\n",
    "                                      args = ( lan_data[language], shuf_order ) )\n",
    "            p[language] = p_lan\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        for language in language_list:\n",
    "            lan_data[language] = p[language].get()\n",
    "    return lan_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_language_dictionary( language, sentences,\n",
    "                               word_to_idx_dict, idx_to_word_dict,\n",
    "                               threshold = 0 ):\n",
    "    # Generate dictionary for each language\n",
    "    word_to_idx_dict_lan = {\"<PAD>\": 0, \"<S>\": 1, \"</S>\": 2, \"<UNK>\": 3}\n",
    "    idx_to_word_dict_lan = {0: \"<PAD>\", 1: \"<S>\", 2: \"</S>\", 3: \"<UNK>\"}\n",
    "\n",
    "    # Count words\n",
    "    word_count = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word_count:\n",
    "                word_count[word] = 0\n",
    "            word_count[word] += 1\n",
    "\n",
    "    # Replace words to <UNK>\n",
    "    for word, count in word_count.items():\n",
    "        if count <= threshold:\n",
    "            word = \"<UNK>\"\n",
    "        if word not in word_to_idx_dict_lan:\n",
    "            idx = len( word_to_idx_dict_lan )\n",
    "            word_to_idx_dict_lan[word] = idx\n",
    "            idx_to_word_dict_lan[idx] = word\n",
    "    \n",
    "    word_to_idx_dict[language] = word_to_idx_dict_lan\n",
    "    idx_to_word_dict[language] = idx_to_word_dict_lan\n",
    "\n",
    "def build_dictionary( language_data, threshold = 0 ):\n",
    "    \"\"\"Build dictionary for each language\n",
    "\n",
    "    Args:\n",
    "        language_data: a dictionary contains sentences of each language.\n",
    "                       Its structure is:\n",
    "\n",
    "                       {language A: [[word1, word2, ...], [...], ...],\n",
    "                        language B: ...}\n",
    "        threshold: a integer represents threshold. If the number of a word\n",
    "                   is less than threshold, it will be replaced by <UNK>.\n",
    "\n",
    "    Returns:\n",
    "        word_to_idx_dict: a dictionary converts word to index. Its structure is:\n",
    "\n",
    "                          {language A: {word A: index A, word B: ..., ...},\n",
    "                           language B: ...}.\n",
    "\n",
    "        idx_to_word_dict: a dictionary converts index to word. Its structure is:\n",
    "\n",
    "                          {language A: {index A: word A, index B: ..., ...},\n",
    "                           language B: ...}.\n",
    "    \"\"\"\n",
    "    manager = multiprocessing.Manager()\n",
    "    word_to_idx_dict = manager.dict()\n",
    "    idx_to_word_dict = manager.dict()\n",
    "    p = []\n",
    "    for language, sentences in language_data.items():\n",
    "        print( \"Building \" + language + \" language dictionary...\" )\n",
    "        p_lan = multiprocessing.Process( target = build_language_dictionary,\n",
    "                                         args = ( language, sentences,\n",
    "                                                  word_to_idx_dict, idx_to_word_dict,\n",
    "                                                  threshold ) )\n",
    "        p.append( p_lan )\n",
    "        p_lan.start()\n",
    "    for p_lan in p:\n",
    "        p_lan.join()\n",
    "    word_to_idx_dict = dict( word_to_idx_dict )\n",
    "    idx_to_word_dict = dict( idx_to_word_dict )\n",
    "    return word_to_idx_dict, idx_to_word_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict( dt, file_name, path = \"dicts/\" ):\n",
    "    \"\"\"Save dictionary to file\n",
    "\n",
    "    Args:\n",
    "        dt: the dictionary to save.\n",
    "        file_name: the name of file to save.\n",
    "        path: the path of file to save.\n",
    "    \"\"\"\n",
    "    print( \"Saving to \" + path + file_name + \"...\" )\n",
    "    file_name = path + file_name\n",
    "    with open( file_name, \"w\" ) as f:\n",
    "        jsn = json.dumps( dt )\n",
    "        f.write( jsn )\n",
    "\n",
    "def sort_by_ori_lan( data ):\n",
    "    \"\"\"Sort dictionary by length of original language\n",
    "\n",
    "    Args:\n",
    "        data: a dictionary contains sentences of each language.  Its structure is:\n",
    "\n",
    "              {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \"\"\"\n",
    "    print( \"Sorting...\" )\n",
    "    lan = list( data.keys() )\n",
    "    tmp = list( zip( data[lan[0]], data[lan[1]] ) )\n",
    "    tmp.sort( key = lambda x: len( x[0] ) )\n",
    "    data[lan[0]], data[lan[1]] = zip( *tmp )\n",
    "\n",
    "def convert_to_index( data, word_to_idx_dict ):\n",
    "    \"\"\"Convert word to index\n",
    "\n",
    "    Args:\n",
    "        data: a dictionary contains sentences of each language.  Its structure is:\n",
    "\n",
    "              {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \n",
    "        word_to_idx_dict: a dictionary converts word to index. Its structure is:\n",
    "\n",
    "                          {language A: {word A: index A, word B: ..., ...},\n",
    "                           language B: ...}.\n",
    "    \"\"\"\n",
    "    print( \"Coverting to index...\" )\n",
    "    lan_list = list( data.keys() )\n",
    "    for lan in lan_list:\n",
    "        for sentence in data[lan]:\n",
    "            for i in range( len( sentence ) ):\n",
    "                word = sentence[i]\n",
    "                if word not in word_to_idx_dict[lan]:\n",
    "                    word = \"<UNK>\"\n",
    "                sentence[i] = word_to_idx_dict[lan][word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer( word_to_vec_map, word_to_index ):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained word2vec 300-dimensional vectors.\n",
    "\n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their word2vec vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    vocab_len = len(word_to_index) + 1 # adding 1 to fit Keras embedding (requirement)\n",
    "    if EMB:\n",
    "        emb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "    else:\n",
    "        emb_dim = 1\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape\n",
    "    # (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros( ( vocab_len, emb_dim ) )\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector\n",
    "    # representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        if EMB:\n",
    "            if word not in word_to_index:\n",
    "                emb_matrix[index, :] = np.random.random( ( 1, 300 ) )\n",
    "            else:\n",
    "                emb_matrix[index, :] = word_to_vec_map[word]\n",
    "        else:\n",
    "            emb_matrix[index, :] = index\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable.\n",
    "    # Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding( vocab_len, emb_dim, trainable = False, mask_zero = True )\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer.\n",
    "    # Do not modify the \"None\".\n",
    "    embedding_layer.build( ( None, ) )\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights( [emb_matrix] )\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_batch( data, batch_size = 64, min_length = 3, max_length = 32 ):\n",
    "    print( \"Converting to batches...\" )\n",
    "    lan_list = list( data.keys() )\n",
    "    tdata = {}\n",
    "    for lan in lan_list:\n",
    "        if lan not in tdata:\n",
    "            tdata[lan] = []\n",
    "    print( type( data[lan_list[0]][0][0] ),\n",
    "           isinstance( data[lan_list[0]][0][0], str ) )\n",
    "    for i in range( 0, len( data[lan_list[0]] ) + batch_size, batch_size ):\n",
    "        lan = {}\n",
    "        max_len = {}\n",
    "        for language in lan_list:\n",
    "            lan[language] = data[language][i: i + batch_size]\n",
    "            max_len[language] = 0\n",
    "        # Calculate max length of valid sentences\n",
    "        n = 0\n",
    "        for j in range( len( lan[lan_list[0]] ) ):\n",
    "            length = len( lan[lan_list[0]][j] )\n",
    "            if length < min_length or max_length < length:\n",
    "                continue\n",
    "            for language in lan_list:\n",
    "                max_len[language] = max( max_len[language], len( lan[language][j] ) )\n",
    "            n += 1\n",
    "        # If there is no sentence valid, ignore the batch\n",
    "        if n == 0:\n",
    "            continue\n",
    "        np_lan = {}\n",
    "        for language in lan_list:\n",
    "            if isinstance( data[lan_list[0]][0][0], str ):\n",
    "                dtype = np.unicode_\n",
    "                np_lan[language] = np.empty( ( n, max_len[language] ), dtype = dtype )\n",
    "                np_lan[language][:] = \"\"\n",
    "            else:\n",
    "                np_lan[language] = np.zeros( ( n, max_len[language] ) )\n",
    "        n = 0\n",
    "        for j in range( len( lan[lan_list[0]] ) ):\n",
    "            length = len( lan[lan_list[0]][j] )\n",
    "            if length < min_length or max_length < length:\n",
    "                continue\n",
    "            for language in lan_list:\n",
    "                for k in range( len( lan[language][j] ) ):\n",
    "                    np_lan[language][n, k] = lan[language][j][k]\n",
    "            n += 1\n",
    "        for language in lan_list:\n",
    "            tdata[language].append( np_lan[language] )\n",
    "    for language in lan_list:\n",
    "        data[language] = tdata[language]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_seq2seq( input_vocab_size, output_vocab_size,\n",
    "                    encoder_embedding, decoder_embedding,\n",
    "                    hidden_dim = 128, word_vec_dim = 300,\n",
    "                    name = \"baseline\" ):\n",
    "    ### Encoder-Decoder for train ###\n",
    "    \n",
    "    # Encoder\n",
    "    encoder = LSTM( hidden_dim, return_state = True,\n",
    "                    name = name + \"_encoder_lstm\" )\n",
    "    encoder_input = Input( shape = ( None, ),\n",
    "                           name = name + \"_encoder_input\" )\n",
    "    \n",
    "    encoder_input_emb   = encoder_embedding( encoder_input )\n",
    "    _, state_h, state_c = encoder( encoder_input_emb )\n",
    "    encoder_state       = [state_h, state_c]\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = LSTM( hidden_dim, return_state = True, return_sequences = True,\n",
    "                    name = name + \"_decoder_lstm\" )\n",
    "    decoder_dense = Dense( output_vocab_size, activation = \"softmax\",\n",
    "                           name = name + \"_decoder_output\" )\n",
    "    decoder_input = Input( shape = ( None, ),\n",
    "                           name = name + \"_decoder_input\" )\n",
    "    \n",
    "    decoder_input_emb = decoder_embedding( decoder_input )\n",
    "    decoder_output, state_h, state_c = decoder( decoder_input_emb,\n",
    "                                                initial_state = encoder_state )\n",
    "    decoder_output    = decoder_dense( decoder_output )\n",
    "    \n",
    "    # Model\n",
    "    model = Model( inputs = [encoder_input, decoder_input],\n",
    "                   outputs = decoder_output,\n",
    "                   name = name )\n",
    "    model.compile( optimizer = 'adam', loss = \"categorical_crossentropy\" )\n",
    "    \n",
    "    ### Encoder-Decoder for generation ###\n",
    "    \n",
    "    # Encoder Model\n",
    "    encoder_model   = Model( inputs  = encoder_input,\n",
    "                             outputs = encoder_state,\n",
    "                             name = name + \"_encoder\" )\n",
    "    \n",
    "    # Decoder Model\n",
    "    decoder_state_h = Input( shape = ( hidden_dim, ), name = name + \"_state_h\" )\n",
    "    decoder_state_c = Input( shape = ( hidden_dim, ), name = name + \"_state_c\" )\n",
    "    decoder_state_input = [decoder_state_h, decoder_state_c]\n",
    "    decoder_output, state_h, state_c = decoder( decoder_input_emb,\n",
    "                                                initial_state = decoder_state_input )\n",
    "    decoder_state   = [state_h, state_c]\n",
    "    decoder_output  = decoder_dense( decoder_output )\n",
    "    decoder_model   = Model( inputs  = [decoder_input] + decoder_state_input,\n",
    "                             outputs = [decoder_output] + decoder_state,\n",
    "                             name = name + \"_decoder\" )\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentences( data, encoder_model, decoder_model, max_len,\n",
    "                         word_to_idx_dict, idx_to_word_dict, language_list ):\n",
    "    \"\"\"Generate sentences based on given sentences\n",
    "\n",
    "    Args:\n",
    "        data: a list of dev data sentences. Its structure is:\n",
    "\n",
    "              [[word1, word2, ...], [...], ...]\n",
    "\n",
    "        encoder_model: encoder part of seq2seq model.\n",
    "        decoder_model: decoder (generate) part of seq2se1 model.\n",
    "        max_len: a interger represents the max length of generated (translated)\n",
    "                 sentence.\n",
    "        word_to_idx_dict: a dictionary converts word to index. Its structure is:\n",
    "\n",
    "                          {language A: {word A: index A, word B: ..., ...},\n",
    "                           language B: ...}.\n",
    "\n",
    "        idx_to_word_dict: a dictionary converts index to word. Its structure is:\n",
    "\n",
    "                          {language A: {index A: word A, index B: ..., ...},\n",
    "                           language B: ...}.\n",
    "\n",
    "    Returns:\n",
    "        sentences: a list of generated (translated) sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    lan_list = language_list\n",
    "    for sentence in data:\n",
    "        init = \"<S>\"\n",
    "        cnt = 0\n",
    "        words = []\n",
    "        state = encoder_model.predict( sentence )\n",
    "        while init != \"</S>\" and cnt <= max_len + 1:\n",
    "            index = np.array( [word_to_idx_dict[lan_list[1]][init]] ).reshape( ( 1, 1 ) )\n",
    "            indeces, state_h, state_c = decoder_model.predict( [index] + state )\n",
    "            index = np.argmax( indeces[0, -1, :] )\n",
    "            init = idx_to_word_dict[lan_list[1]][index]\n",
    "            state = [state_h, state_c]\n",
    "            words.append( init )\n",
    "            cnt += 1\n",
    "        print( words[:-1] )\n",
    "        sentences.append( words[:-1] )\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_name = \"baseline\"\n",
    "    language_list = [\"chinese\", \"english\"] # [ori_lan, tar_lan]\n",
    "    batch_size = 64\n",
    "    max_length = 32\n",
    "\n",
    "    data = get_data( \"../data/test/\", language_list, shuffle = False )\n",
    "    word_to_idx_dict, idx_to_word_dict = build_dictionary( data, 15 )\n",
    "    if EMB:\n",
    "        word_to_vec = load_word2vec( \"../data/word2vec/\", language_list )\n",
    "    else:\n",
    "        word_to_vec = word_to_idx_dict\n",
    "    print( len( word_to_idx_dict[language_list[0]] ), len( word_to_idx_dict[language_list[1]] ) )\n",
    "    save_dict( word_to_idx_dict, \"word_to_idx.json\" )\n",
    "    save_dict( idx_to_word_dict, \"idx_to_word.json\" )\n",
    "    input_vocab_size = len( word_to_idx_dict[language_list[0]] )\n",
    "    output_vocab_size = len( word_to_idx_dict[language_list[1]] )\n",
    "\n",
    "    print( \"Building Model\" )\n",
    "    encoder_embedding = pretrained_embedding_layer(\n",
    "                            word_to_vec[language_list[0]], word_to_idx_dict[language_list[0]] )\n",
    "    decoder_embedding = pretrained_embedding_layer(\n",
    "                            word_to_vec[language_list[1]], word_to_idx_dict[language_list[1]] )\n",
    "    model, encoder_model, decoder_model = simple_seq2seq( input_vocab_size,  output_vocab_size,\n",
    "                                                          encoder_embedding, decoder_embedding,\n",
    "                                                          name = model_name )\n",
    "\n",
    "    sort_by_ori_lan( data )\n",
    "    tdata = copy.deepcopy( data )\n",
    "    for lan in language_list:\n",
    "        print( len( data[lan] ), len( tdata[lan] ) )\n",
    "    convert_to_index( tdata, word_to_idx_dict )\n",
    "    convert_to_batch( data )\n",
    "    convert_to_batch( tdata )\n",
    "    for lan in language_list:\n",
    "        print( len( data[lan] ) )\n",
    "\n",
    "    print( \"Training Model\" )\n",
    "    n = 0\n",
    "    losses = []\n",
    "    for epoch in range( 0 ):\n",
    "        for i in range( len( data[language_list[0]] ) ):\n",
    "            label = K.utils.to_categorical( tdata[language_list[1]][i],\n",
    "                                            output_vocab_size )\n",
    "            loss = model.train_on_batch( [data[language_list[0]][i],\n",
    "                                          data[language_list[1]][i]],\n",
    "                                         label )\n",
    "            losses.append( loss )\n",
    "            print( n, loss )\n",
    "            n += 1\n",
    "            if n % 5000 == 0:\n",
    "                model.save_weights( \"models/model_weights_\" + str( n ) + \".h5\" )\n",
    "    with open( \"losses.txt\", \"w\" ) as f:\n",
    "        for loss in losses:\n",
    "            f.write( str( loss ) + \"\\n\" )\n",
    "\n",
    "    data = get_data( \"../data/test/\", language_list, shuffle = False )\n",
    "    convert_to_index( data, word_to_idx_dict )\n",
    "    convert_to_batch( data, batch_size = 1, min_length = 0, max_length = 10000 )\n",
    "    print( \"Generating sentences\" )\n",
    "    translated_sentences = translate_sentences( data[language_list[0]],\n",
    "                                                encoder_model, decoder_model,\n",
    "                                                max_length,\n",
    "                                                word_to_idx_dict, idx_to_word_dict,\n",
    "                                                language_list )\n",
    "    with open( \"translated_sentence.txt\", \"w\" ) as f:\n",
    "        for sentence in translated_sentences:\n",
    "            f.write( sentence + \"\\n\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
