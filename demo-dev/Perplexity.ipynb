{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Add-Lambda smoothing function on language model.\n",
    "\"\"\"Load language model\n",
    "\n",
    "Load language model from folder \"lm\" and save them into dictionary \"lm\".\n",
    "\n",
    "Args:\n",
    "    loadPath: language model load path.\n",
    "    encoding: language model files' encoding\n",
    "\n",
    "Returns:\n",
    "    lm: a tensorflow HashTable of language model. Its structure is:\n",
    "    \n",
    "    {\"unigram\": unigram,\n",
    "     \"bigram\" : bigram,\n",
    "     \"trigram\": trigram}.\n",
    "\"\"\"\n",
    "def loadLM( loadPath = \"./lm\", encoding = \"utf-8\" ):\n",
    "    tlm = {}\n",
    "    ngram = [\"unigram\", \"bigram\", \"trigram\"]\n",
    "    # load unigram, bigram, and trigram\n",
    "    for name in ngram:\n",
    "        with open( loadPath + \"/\" + name, \"r\", encoding = encoding ) as f:\n",
    "            ngram = {}\n",
    "            total = 0\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                kv = line.split( ' ' )\n",
    "                if len( kv ) > 1:\n",
    "                    k = ' '.join( kv[:-1] )\n",
    "                    v = kv[-1]\n",
    "                    ngram[k] = int( v )\n",
    "                else:\n",
    "                    pass\n",
    "                line = f.readline()\n",
    "            tlm[name] = tf.HashTable( tf.KeyValueTensorInitializer(\n",
    "                                        ngram.keys(), ngram.values() ), 0 )\n",
    "    lm = tf.HashTable( tf.KeyValueTensorInitializer( tlm.keys(), tlm.values() ), 0 )\n",
    "    return lm\n",
    "\n",
    "\"\"\"Add lambda smoothing\n",
    "\n",
    "P(w_{n}|w_{n-1}w_{n-2}) = ( c(w_{n-1}w_{n-2}, w_{n}) + lambda ) /\n",
    "                            ( c(w_{n-1}w_{n-1}) + lambda * V )\n",
    "\n",
    "Args:\n",
    "    lm: a tensorflow HashTable of language model. Its structure is:\n",
    "    \n",
    "    {\"unigram\": unigram,\n",
    "     \"bigram\" : bigram,\n",
    "     \"trigram\": trigram}.\n",
    "    \n",
    "    lambdas: a generator of lambdas which generate a dictionary of lambdas\n",
    "             for unigram, bigram, trigram each time. For example:\n",
    "    \n",
    "    {1:0.1, 2:0.1, 3:0.8}\n",
    "    \n",
    "    s: a list of words wating for calculating unigram, bigram, and trigram.\n",
    "\n",
    "Returns:\n",
    "    p: a double number represents the final probability of P(w_{n}|w_{n-2}w_{n-1}).\n",
    "\"\"\"\n",
    "def addLambda( lm, lambdas, s ):\n",
    "    s = tf.slice( s, [-1] )\n",
    "    if tf.equal( lm.lookup( \"trigram\" ).lookup( s ), 0 ):\n",
    "        cnt1 = 0\n",
    "    else:\n",
    "        cnt1 = lm.lookup( \"trigram\" ).lookup( s )\n",
    "    s = ' '.join( s[:-1] )\n",
    "    if s not in lm[\"bigram\"][\"c\"]:\n",
    "        cnt2 = 0\n",
    "    else:\n",
    "        cnt2 = lm[\"bigram\"][\"c\"][s]\n",
    "    p = ( cnt1 + lambdas[3] ) / ( cnt2 + len( lm[\"bigram\"][\"c\"] ) * lambdas[3] )\n",
    "    return p\n",
    "\n",
    "\"\"\"Calculate perplexity\n",
    "\n",
    "PP(W) = P(w_1w_2 ... w_n)^(-1/n)\n",
    "      = 2^{-1 / n * sum_{i=1:n}(log2(LM(w_i|w_{i-2}w_{i-1})))}\n",
    "\n",
    "Note: Since here is no <SOS> and <EOS> in language model, n would be the length of\n",
    "      the content - 2.\n",
    "\n",
    "Args:\n",
    "    contents: a tensor contains words in a sentences. Each line represents a sentence.\n",
    "    lm: a tensorflow HashTable of language model. Its structure is:\n",
    "    \n",
    "    {\"unigram\": unigram,\n",
    "     \"bigram\" : bigram,\n",
    "     \"trigram\": trigram}.\n",
    "\n",
    "    **kwargs:\n",
    "        func: smoothing function name on calculating P(w_i|w_{i-2}w_{i-1}), including\n",
    "              func = \"Interplotation\" and func = \"AddLambda\".\n",
    "        \n",
    "        lambdas: a dictionary of lambda for interplotation or addLambda. Its structure is:\n",
    "    \n",
    "        {1: lambdaForUnigram, 2:lambdaForBigram, 3:lambdaForTrigram}\n",
    "        \n",
    "        When using addLambda function, only need to feed one specific lambda.\n",
    "    \n",
    "Returns:\n",
    "    ppw: a double number represents the perplexity of the content.\n",
    "\n",
    "Raise:\n",
    "    KeyError: an error when trying to find smoothing function.\n",
    "\"\"\"\n",
    "def perplexity( contents, lm, **kwargs ):\n",
    "    numOfSentence = contents.shape[0]\n",
    "    length = contents.shape[1]\n",
    "    logP = tf.Variable( 0, name = \"logP\" )\n",
    "    if( length <= 2 ):\n",
    "        raise Exception( \"Too short content.\" )\n",
    "    if \"func\" in kwargs:\n",
    "        if kwargs[\"func\"] == \"AddLambda\":\n",
    "            for i in range( numOfSentence ):\n",
    "                content = tf.strided_clise( contents, [i], [i + 1] )\n",
    "                for i in range( length - 2 ):\n",
    "                    p = tf.Variable( 0, name = \"p\" )\n",
    "                    p = addLambda( lm, kwargs[\"lambdas\"], tf.strided_slice( content, [:, i], [:, i + 3] ) )\n",
    "                    logP = logP + tf.math.log( p )\n",
    "        else:\n",
    "            raise Exception( \"Cannot find the smoothing function.\" )\n",
    "    else:\n",
    "        raise Exception( \"No smoothing function.\" )\n",
    "    logP = logP * -1 / ( length - 2 )\n",
    "    ppw = tf.math.exp( logP )\n",
    "    return ppw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1 2]]\n",
      "[[3 4]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable( [[1, 2], [3, 4]] )\n",
    "c = tf.strided_slice( a, [0], [1] )\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run( init )\n",
    "    print( sess.run( a ) )\n",
    "    for i in range( a.shape[0] ):\n",
    "        print( sess.run( tf.strided_slice( a, [i], [i + 1] ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
