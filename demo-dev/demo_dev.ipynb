{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\"\"\" Abstract\n",
    "\n",
    "The demo is a prototype of the project model. Codes and structure here could change in the future.\n",
    "The main point of codes below is to run on local computer and test whether it works on small scale of data.\n",
    "\n",
    "Now:\n",
    "    Prove that original model is inaccessible. Start to implemente original RNN-LSTM and treat it as baseline.\n",
    "\n",
    "TODO:\n",
    "    1. Try new way to approach the unsupervised method.\n",
    "    2. Try to understand how to control gradient in Tensorflow.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import threading\n",
    "import math\n",
    "import queue\n",
    "import multiprocessing\n",
    "import h5py\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Lambda, concatenate, Multiply\n",
    "\n",
    "K.backend.clear_session()\n",
    "KTF.set_session( tf.Session( config = tf.ConfigProto( device_count = {'gpu':0} ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Get train data from path\n",
    "\n",
    "Read paralelled train data from files for each language and save to a dictionary.\n",
    "\n",
    "Args:\n",
    "    dataPath: file path of train data. Default value is \"../../Data/train/\".\n",
    "    langList: language list. Indicating which language data will be included in train data.\n",
    "              Default value is [\"Chinese\", \"English\"].\n",
    "    encoding: encoding of each file in train directory.\n",
    "    ratio: propotion of train data, others will be treat as dev data. Default value is 0.98.\n",
    "\n",
    "Returns:\n",
    "    trainData: a dictionary of train data sentences of each language. Its structure is:\n",
    "    \n",
    "               {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \n",
    "    devData: a dictionary of dev data sentences of each language. Its structure is:\n",
    "    \n",
    "               {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "\"\"\"\n",
    "def getTrainData( dataPath = \"../../Data/train/\", lanList = [\"chinese\", \"english\"],\n",
    "                  encoding = \"UTF-8\", ratio = 0.98 ):\n",
    "    trainData = {}\n",
    "    devData   = {}\n",
    "    data = {}\n",
    "    for lan in lanList:\n",
    "        if lan not in data:\n",
    "            data[lan] = []\n",
    "        print( \"Reading \" + lan + \" files...\" )\n",
    "        files = os.listdir( dataPath + lan + \"/\" )\n",
    "        for file in files:\n",
    "            with open( dataPath + lan + \"/\" + file, encoding = encoding ) as f:\n",
    "                line = f.readline()\n",
    "                while line:\n",
    "                    wordList = line.split()\n",
    "                    data[lan].append( [\"<S>\"] + wordList + [\"</S>\"] )\n",
    "                    line = f.readline()\n",
    "    noOfSentences = len( data[lanList[0]] )\n",
    "    arr = [i for i in range( noOfSentences )]\n",
    "    random.shuffle( arr )\n",
    "    for lan in lanList:\n",
    "        if lan not in trainData:\n",
    "            trainData[lan] = []\n",
    "        if lan not in devData:\n",
    "            devData[lan] = []\n",
    "        data[lan] = np.array( data[lan] )[arr].tolist()\n",
    "        noOfTrainData = int( noOfSentences * ratio )\n",
    "        devData[lan]   = data[lan][noOfTrainData:]\n",
    "        trainData[lan] = data[lan][:noOfTrainData]\n",
    "        print( trainData[lan][:5] )\n",
    "    return trainData, devData\n",
    "\n",
    "\"\"\" Generate dictionary and preprocess setences for each language\n",
    "\n",
    "Generate dictionary for each language and convert word to corresponding index.\n",
    "Here we set two dictionaries to speed up the whole program.\n",
    "\n",
    "Moreover, the function will replace word in sentences to index automatically.\n",
    "\n",
    "Args:\n",
    "    data: a dictionary contains sentences of each language.  Its structure is:\n",
    "    \n",
    "          {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \n",
    "    threshold: a word will be replace with <UNK> if frequency of a word is\n",
    "               less than threshold. If the value is less than 1, it means\n",
    "               no need to replace any word to <UNK>. Default value is 0.\n",
    "\n",
    "Returns:\n",
    "    wordNumDict: a dictionary which can convert words to index in each language.\n",
    "                 Its structure is:\n",
    "                 \n",
    "                 {language A: {word A: index A, word B: ..., ...}, language B: ..., ...}.\n",
    "    \n",
    "    numWordDict: a dictionary which can convert index to word in each language.\n",
    "                 Its structure is:\n",
    "                 \n",
    "                 {language A: {index A: word A, index B: ..., ...}, language B: ..., ...}.\n",
    "\"\"\"\n",
    "def generateDict( data, threshold = 0 ):\n",
    "    wordNumDict = {}\n",
    "    numWordDict = {}\n",
    "    for lan, sentences in data.items():\n",
    "        print( \"Generating \" + lan + \" dictionary...\" )\n",
    "        wordCount = {}\n",
    "        if lan not in wordNumDict:\n",
    "            # Add special word to dictionary\n",
    "            wordNumDict[lan] = {\"<PAD>\": 0, \"<S>\": 1, \"</S>\": 2, \"<UNK>\": 3}\n",
    "        if lan not in numWordDict:\n",
    "            # Add special word to dictionary\n",
    "            numWordDict[lan] = {0: \"<PAD>\", 1: \"<S>\", 2: \"</S>\", 3: \"<UNK>\"}\n",
    "        \n",
    "        # Count word frequency\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in wordCount:\n",
    "                    wordCount[word] = 0\n",
    "                wordCount[word] += 1\n",
    "        \n",
    "        # Find and replace with <UNK>\n",
    "        for sentence in sentences:\n",
    "            for i in range( len( sentence ) ):\n",
    "                word = sentence[i]\n",
    "                if wordCount[word] < threshold:\n",
    "                    word = \"<UNK>\"\n",
    "                if word not in wordNumDict[lan]:\n",
    "                    number = len( wordNumDict[lan] )\n",
    "                    wordNumDict[lan][word] = number\n",
    "                    numWordDict[lan][number] = word\n",
    "                sentence[i] = wordNumDict[lan][word]\n",
    "    return wordNumDict, numWordDict\n",
    "\n",
    "\"\"\"Sort dictionary by length of original language\n",
    "\n",
    "Args:\n",
    "    data: a dictionary contains sentences of each language.  Its structure is:\n",
    "    \n",
    "          {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \n",
    "    lan: a list of language. The first one is the original language, the second\n",
    "         one is the target language. For example:\n",
    "\n",
    "         [language A, language B].\n",
    "\n",
    "Returns:\n",
    "    None.\n",
    "\"\"\"\n",
    "def sortByOriLan( data, lan = [\"chinese\", \"english\"] ):\n",
    "    print( \"Sorting data...\" )\n",
    "    tmp = list( zip( data[lan[0]], data[lan[1]] ) )\n",
    "    tmp.sort( key = lambda x: len( x[0] ) )\n",
    "    data[lan[0]], data[lan[1]] = zip( *tmp )\n",
    "\n",
    "\"\"\"Number to One-hot\n",
    "\n",
    "Only convert sentences which length are small than 30.\n",
    "\n",
    "Args:\n",
    "    data: a dictionary contains sentences of each language.  Its structure is:\n",
    "    \n",
    "          {language A: [[word1, word2, ...], [...], ...], language B: ...}.\n",
    "    \n",
    "    lan: a list of language. The first one is the original language, the second\n",
    "         one is the target language. For example:\n",
    "\n",
    "         [language A, language B].\n",
    "\n",
    "Returns:\n",
    "    status: boolean value. True represents successfully extract batch data. False\n",
    "            represents extract nothing from original data.\n",
    "    ndata: data like dictionary.\n",
    "\"\"\"\n",
    "def toCategory( data, lan ):\n",
    "    maxlLan0 = 0\n",
    "    maxlLan1 = 0\n",
    "    n = 0\n",
    "    for i in range( len( data[lan[0]] ) ):\n",
    "        if len( data[lan[0]][i] ) <= 32:\n",
    "            n += 1\n",
    "            maxlLan0 = max( maxlLan0, len( data[lan[0]][i] ) )\n",
    "            maxlLan1 = max( maxlLan1, len( data[lan[1]][i] ) )\n",
    "    if n == 0:\n",
    "        return False, [], []\n",
    "    lan0 = np.zeros( ( n, maxlLan0 ) )\n",
    "    lan1 = np.zeros( ( n, maxlLan1 ) )\n",
    "    n = 0\n",
    "    for i in range( len( data[lan[0]] ) ):\n",
    "        if len( data[lan[0]][i] ) <= 32:\n",
    "            for j in range( len( data[lan[0]][i] ) ):\n",
    "                lan0[n, j] = data[lan[0]][i][j]\n",
    "            for j in range( len( data[lan[1]][i] ) ):\n",
    "                lan1[n, j] = data[lan[1]][i][j]\n",
    "            n += 1\n",
    "    data[lan[0]] = lan0\n",
    "    data[lan[1]] = lan1\n",
    "    return True, data\n",
    "\n",
    "def toCategoryWrap( args ):\n",
    "    return toCategory( *args )\n",
    "\n",
    "\"\"\" Simple Seqence to Sequence Implementation\n",
    "\n",
    "A simple implementation of Sequence to Sequence model. It works as baseline\n",
    "\n",
    "Args:\n",
    "    input_dim:  dimension of input word vector.\n",
    "    output_dim: dimension of output word vector.\n",
    "    hidden_dim: dimension of hidden states vector.\n",
    "    output_vocab_size: size of output language vocabulary size.\n",
    "    input_vocab_size:  size of input  language vocabulary size.\n",
    "    word_vec_dim: dimension of word-vector.\n",
    "    name: name of the model.\n",
    "\n",
    "Returns:\n",
    "    model: the whole model of simple Seq2Seq model.\n",
    "\n",
    "\"\"\"\n",
    "def simpleSeq2Seq( output_vocab_size, input_vocab_size, hidden_dim = 128,\n",
    "                   word_vec_dim = 300, name = \"demo\" ):\n",
    "    embedding_encoder  = Embedding( output_dim = word_vec_dim, input_dim = input_vocab_size,\n",
    "                                 name = name + \"_encoder_embedding\", mask_zero = True ) # \n",
    "    embedding_decoder = Embedding( output_dim = word_vec_dim, input_dim = output_vocab_size,\n",
    "                                 name = name + \"_decoder_embedding\", mask_zero = True ) # \n",
    "    # Encoder\n",
    "    encoder_input     = Input( shape = ( None, ), name = name + \"_encoder_input\" )\n",
    "    # change when using pre-trained embedding trainable= False\n",
    "    encoder           = LSTM( hidden_dim, return_state = True )\n",
    "    encoder_input_emb = embedding_encoder( encoder_input )\n",
    "    _, state_h, state_c = encoder( encoder_input_emb )\n",
    "    state_encoder     = [state_h, state_c]\n",
    "    # Decoder\n",
    "    decoder = LSTM( hidden_dim, return_sequences = True )\n",
    "\n",
    "    decoder_input     = Input( shape = ( None, ), name = name + \"_decoder_input\" )\n",
    "    decoder_input_emb = embedding_decoder( decoder_input )\n",
    "    decoder_outputs   = decoder( decoder_input_emb, initial_state = state_encoder )\n",
    "    decoder_dense     = Dense( output_vocab_size, activation = \"softmax\", name = name + \"_decoder_output\" )\n",
    "    decoder_outputs   = decoder_dense( decoder_outputs )\n",
    "\n",
    "    # Build model\n",
    "    model = Model( inputs = [encoder_input, decoder_input], outputs = decoder_outputs, name = name )\n",
    "    model.compile( optimizer = 'adam', loss = \"categorical_crossentropy\" )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chinese files...\n",
      "Reading english files...\n",
      "[['<S>', '上海', '信息', '交互', '网', '采用', '的', '网络', '技术', '与', '当今', '国际', '互联', '技术', '的', '发展', '同步', ',', '是', '目前', '世界', '先进', '网络', '技术', '在', '上海', '的', '成功', '运用', '</S>'], ['<S>', '章启月', '说', ',', '中国', '在', '国际', '反恐', '斗争', '中', '发挥', '了', '建设性', '作用', ',', '坚决', '反对', '一切', '形式', '的', '恐怖主义', ',', '积极', '参与', '国际', '反恐', '合作', ',', '受到', '国际', '社会', '的', '广泛', '好评', '</S>'], ['<S>', '李鹏', '说', ':', '\"', '马里', '是', '最', '早', '同', '中国', '建交', '的', '非洲', '国家', '之一', '中', '马', '友好', '关系', '始终', '健康', '顺利', '地', '发展', ',', '双方', '的', '合作', '卓有', '成效', ',', '我们', '希望', '两', '国', '的', '友好', '合作', '关系', '长期', '稳定', '地', '发展', '下去', '\"', '</S>'], ['<S>', '据', '统计', ',', '19', '月', '全', '国', '乡', '及', '乡', '以上', '工业', '企业', '累计', '完成', '工业', '增加值', '11558', '亿', '元', ',', '比', '上', '年', '同', '期', '增长', '百分之十六点六', '</S>'], ['<S>', '这', '位', '不', '愿', '透露', '姓名', '的', '官员', '表示', ',', '在', '最', '坏', '的', '情况', '下', ',', '短期', '「', '将', '造成', '九', '(', '百万', ')', '至', '一千两百万', '人', '丧生', ',', '二', '(', '百万', ')', '至', '六百万', '人', '受伤', '」', '。', '</S>']]\n",
      "[['<S>', 'users', 'of', 'the', 'net', 'will', 'be', 'able', 'to', 'share', 'various', 'kinds', 'of', 'information', 'covering', 'education', ',', 'science', ',', 'medical', 'care', ',', 'culture', ',', 'markets', ',', 'government', 'policies', 'and', 'regulations', ',', 'and', 'life', 'matters', '</S>'], ['<S>', 'zhang', 'said', 'at', 'a', 'routine', 'press', 'conference', 'that', 'china', 'has', 'played', 'a', 'constructive', 'role', 'in', 'the', 'international', 'counter', 'terrorism', 'struggle', ',', 'firmly', 'opposed', 'terrorism', 'of', 'any', 'form', ',', 'took', 'an', 'active', 'part', 'in', 'international', 'anti', 'terrorism', 'cooperation', ',', 'and', 'won', 'numerous', 'compliments', 'from', 'the', 'international', 'community', '</S>'], ['<S>', 'li', 'pointed', 'out', 'that', 'the', 'sino', 'mali', 'friendship', 'has', 'developed', 'along', 'a', 'smooth', 'and', 'healthy', 'track', ',', 'and', 'the', 'cooperation', 'between', 'the', 'two', 'countries', 'has', 'been', 'successful', 'he', 'added', 'that', 'china', 'hopes', 'the', 'ties', 'of', 'friendly', 'cooperation', 'between', 'the', 'two', 'countries', 'will', 'continue', 'to', 'develop', 'stably', 'and', 'over', 'a', 'long', 'period', 'of', 'time', '</S>'], ['<S>', 'cumulative', 'figures', 'indicated', 'that', 'the', 'industrial', 'added', 'value', 'totaled', '1,155', '8', 'billion', 'yuan', 'in', 'the', 'first', 'nine', 'months', 'this', 'year', ',', 'up', '16', '6', 'percent', 'from', 'the', 'same', 'period', 'of', 'last', 'year', '</S>'], ['<S>', 'the', 'official', ',', 'who', 'declined', 'to', 'be', 'named', ',', 'said', 'that', 'under', 'the', 'worst', 'circumstances', ',', 'in', 'a', 'short', 'period', 'of', 'time', '\"', 'the', 'death', 'toll', 'could', 'range', 'from', '9', '(', 'million', ')', 'to', '12', '-', 'million', ',', 'and', 'another', '2', '(', 'million', ')', 'to', '6', 'million', 'could', 'be', 'injured', '.', '\"', '</S>']]\n",
      "Generating chinese dictionary...\n",
      "Generating english dictionary...\n",
      "Sorting data...\n",
      "10267 9494\n"
     ]
    }
   ],
   "source": [
    "trainData, devData = getTrainData( \"../../Data/test/\" )\n",
    "wordNumDict, numWordDict = generateDict( trainData, threshold = 5 )\n",
    "sortByOriLan( trainData, [\"chinese\", \"english\"] )\n",
    "ivs = len( wordNumDict[\"chinese\"] )\n",
    "ovs = len( wordNumDict[\"english\"] )\n",
    "print( ivs, ovs )\n",
    "\n",
    "#trainData[\"chinese\"] = trainData[\"chinese\"][::-1]\n",
    "#devData[\"chinese\"] = devData[\"chinese\"][::-1]\n",
    "#trainData[\"english\"] = trainData[\"english\"][::-1]\n",
    "#devData[\"english\"] = devData[\"english\"][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "demo_encoder_input (InputLayer) (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "demo_decoder_input (InputLayer) (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "demo_encoder_embedding (Embeddi (None, None, 300)    3080100     demo_encoder_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "demo_decoder_embedding (Embeddi (None, None, 300)    2848200     demo_decoder_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 219648      demo_encoder_embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, None, 128)    219648      demo_decoder_embedding[0][0]     \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "demo_decoder_output (Dense)     (None, None, 9494)   1224726     lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 7,592,322\n",
      "Trainable params: 7,592,322\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print( \"Building model...\" )\n",
    "model = simpleSeq2Seq( output_vocab_size = ovs, input_vocab_size = ivs, name = \"demo\" )\n",
    "model.summary()\n",
    "\n",
    "batch_size = 64\n",
    "losses = []\n",
    "n = 0\n",
    "total = len( trainData[\"chinese\"] )\n",
    "length = len( wordNumDict[\"english\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel processing training data\n",
      "MAP\n"
     ]
    }
   ],
   "source": [
    "print( \"Parallel processing training data\" )\n",
    "# cores = multiprocessing.cpu_count()\n",
    "# pool = multiprocessing.Pool( processes = cores )\n",
    "params = []\n",
    "for i in range( 0, total + batch_size, batch_size ):\n",
    "    # Divide data into batch\n",
    "    tdata = {}\n",
    "    tdata[\"chinese\"] = trainData[\"chinese\"][i:i + batch_size]\n",
    "    tdata[\"english\"] = trainData[\"english\"][i:i + batch_size]\n",
    "    # Combine all params\n",
    "    params.append( [tdata, [\"chinese\", \"english\"]] )\n",
    "print( \"MAP\" )\n",
    "rets = []\n",
    "for param in params:\n",
    "    rets.append( toCategoryWrap( param ) )\n",
    "# for ret in pool.imap_unordered( toCategoryWrap, params ):\n",
    "#     rets.append( ret )\n",
    "# rets = pool.map( toCategoryWrap, params )\n",
    "# pool.close()\n",
    "# pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 3.140e+02 2.950e+02 3.000e+00 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 2.221e+03 3.525e+03 3.000e+00 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 2.336e+03 8.520e+02 8.225e+03 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 2.010e+03 2.950e+02 9.170e+02 2.000e+00]\n",
      " [1.000e+00 3.000e+00 8.520e+02 3.103e+03 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 3.000e+00 1.740e+02 3.000e+00 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 2.030e+02 3.000e+00 4.700e+01 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 2.150e+02 5.800e+01 3.000e+00 2.000e+00]\n",
      " [1.000e+00 5.470e+02 9.128e+03 1.860e+03 2.000e+00]\n",
      " [1.000e+00 3.000e+00 3.000e+00 9.227e+03 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 2.120e+02 2.051e+03 9.379e+03 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 4.735e+03 9.449e+03 6.981e+03 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 2.609e+03 2.950e+02 9.170e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 6.990e+02 5.480e+02 9.170e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 3.000e+00 5.290e+02 3.535e+03 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 2.221e+03 3.525e+03 3.000e+00 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 2.030e+02 3.000e+00 4.700e+01 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]\n",
      " [1.000e+00 7.860e+02 2.361e+03 1.901e+03 2.000e+00]\n",
      " [1.000e+00 1.100e+02 1.435e+03 1.120e+02 2.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print( rets[1][1][\"chinese\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0315 05:48:33.125375 11084 deprecation.py:323] From C:\\Users\\Ruosen\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.157951\n",
      "Training...\n",
      "00  512512 9494\n",
      "\n",
      "2- 0\n",
      "2- 1\n",
      "2- 2\n",
      "2- 3\n",
      "2- 4\n",
      "2- 5\n",
      "2- 6\n",
      "2- 7\n",
      "2- 8\n",
      "2- 9\n",
      "2- 10\n",
      "2- 11\n",
      "2- 12\n",
      "2- 13\n",
      "2- 14\n",
      "2- 15\n",
      "2- 16\n",
      "2- 17\n",
      "2- 18\n",
      "2- 19\n",
      "2- 20\n",
      "2- 21\n",
      "2- 22\n",
      "2- 23\n",
      "2- 24\n",
      "2- 25\n",
      "2- 26\n",
      "2- 27\n",
      "2- 28\n",
      "2- 29\n",
      "2- 30\n",
      "2- 31\n",
      "2- 32\n",
      "2- 33\n",
      "2- 34\n",
      "2- 35\n",
      "2- 36\n",
      "2- 37\n",
      "2- 38\n",
      "2- 39\n",
      "2- 40\n",
      "2- 41\n"
     ]
    }
   ],
   "source": [
    "#model.load_weights( \"Models/old/model_weights_18000.h5\" )\n",
    "#n = 18000\n",
    "\n",
    "# Multi-thread\n",
    "que = queue.Queue( 5 )\n",
    "i = 0\n",
    "lrets = len( rets )\n",
    "\n",
    "inputs = [rets[1][1][\"chinese\"], rets[1][1][\"english\"]]\n",
    "outputs = K.utils.to_categorical( rets[1][1][\"english\"], length )\n",
    "loss = model.train_on_batch( inputs, outputs )\n",
    "print( loss )\n",
    "\n",
    "def putToQueue():\n",
    "    global i, rets, lrets, que, length\n",
    "    print( i, lrets, length )\n",
    "    while i < lrets:\n",
    "        if rets[i][0] == False:\n",
    "            i += 1\n",
    "            continue\n",
    "        if not que.full():\n",
    "            print( \"2-\", i )\n",
    "            que.put( [i, K.utils.to_categorical( rets[i][1][\"english\"], length )] )\n",
    "            i += 1\n",
    "\n",
    "def run():\n",
    "    global i, rets, lrets, que, model, losses, length\n",
    "    print( i, lrets )\n",
    "    while i < lrets or not que.empty():\n",
    "        if not que.empty():\n",
    "            tmp = que.get()\n",
    "            idx = tmp[0]\n",
    "            tmp = tmp[1]\n",
    "            que.task_done()\n",
    "            if rets[idx][0] == False:\n",
    "                continue\n",
    "            ret = rets[idx]\n",
    "            inputs = [ret[1][\"chinese\"], ret[1][\"english\"]]\n",
    "            outputs = tmp\n",
    "            loss = model.train_on_batch( inputs, outputs )\n",
    "            losses.append( loss )\n",
    "            if idx and idx % 3000 == 0:\n",
    "                model.save_weights(\"Models/model_weights_\" + str( idx ) + \".h5\" ) \n",
    "\n",
    "print( \"Training...\" )\n",
    "t1 = threading.Thread( target = putToQueue )\n",
    "t2 = threading.Thread( target = run )\n",
    "t1.start()\n",
    "t2.start()\n",
    "t1.join()\n",
    "t2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for ret in rets:\n",
    "# #    status, newTrainData, td = toCategory( trainData, [\"chinese\", \"english\"], length, i, min( i + batch_size, total ) )\n",
    "#     if ret[0] == False:\n",
    "#         continue\n",
    "#     label = K.utils.to_categorical( ret[1][\"english\"], length )\n",
    "#     loss = model.train_on_batch( [ret[1][\"chinese\"], ret[1][\"english\"]], label )\n",
    "#     n += 1\n",
    "#     print( n, loss )\n",
    "#     if n and n % 3000 == 0:\n",
    "#         model.save_weights(\"Models/model_weights_\" + str( n ) + \".h5\" ) \n",
    "#     losses.append( loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"Models/model_weights_\" + str( lrets ) + \".h5\" ) \n",
    "with open( \"losses\", \"w\" ) as f:\n",
    "    for loss in losses:\n",
    "        f.write( str( loss ) + \"\\n\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
